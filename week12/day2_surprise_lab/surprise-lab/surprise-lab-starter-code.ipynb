{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Recommendations with surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-the-movielens-100k-dataset-from-disk\" data-toc-modified-id=\"Load-the-movielens-100k-dataset-from-disk-1\">Load the movielens-100k dataset from disk</a></span><ul class=\"toc-item\"><li><span><a href=\"#Instantiate-the-algorithm\" data-toc-modified-id=\"Instantiate-the-algorithm-1.1\">Instantiate the algorithm</a></span></li><li><span><a href=\"#Extract-the-model-parameters\" data-toc-modified-id=\"Extract-the-model-parameters-1.2\">Extract the model parameters</a></span></li><li><span><a href=\"#Evaluate-the-model:\" data-toc-modified-id=\"Evaluate-the-model:-1.3\">Evaluate the model:</a></span></li><li><span><a href=\"#Put-the-predictions-in-a-dataframe\" data-toc-modified-id=\"Put-the-predictions-in-a-dataframe-1.4\">Put the predictions in a dataframe</a></span></li><li><span><a href=\"#Correlations-between-predicted-and-true-ratings\" data-toc-modified-id=\"Correlations-between-predicted-and-true-ratings-1.5\">Correlations between predicted and true ratings</a></span></li></ul></li><li><span><a href=\"#Cross-validation,-train-test-split-and-grid-search\" data-toc-modified-id=\"Cross-validation,-train-test-split-and-grid-search-2\">Cross validation, train-test split and grid search</a></span></li><li><span><a href=\"#Slope-One\" data-toc-modified-id=\"Slope-One-3\">Slope One</a></span></li><li><span><a href=\"#KNN-with-Means\" data-toc-modified-id=\"KNN-with-Means-4\">KNN with Means</a></span></li><li><span><a href=\"#Precision@k-and-Recall@k\" data-toc-modified-id=\"Precision@k-and-Recall@k-5\">Precision@k and Recall@k</a></span></li><li><span><a href=\"#Top-n-predictions\" data-toc-modified-id=\"Top-n-predictions-6\">Top-n predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coverage\" data-toc-modified-id=\"Coverage-6.1\">Coverage</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make use of the [surprise package](https://surprise.readthedocs.io/en/stable/index.html), a package dedicated to recommendation systems.\n",
    "\n",
    "`conda install -c conda-forge scikit-surprise`\n",
    "\n",
    "First we will need some data. Download the Movielens 100K dataset from [here](https://grouplens.org/datasets/movielens/). \n",
    "It is a very famous dataset about movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load surprise\n",
    "import surprise as sur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movielens-100k dataset from disk\n",
    "\n",
    "With the above command we could load the data in a simplified and already prepared way. As reading and preparing other files is not that straight-forward, we will rather load the file from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\n",
    "    '~/.surprise_data/ml-100k/ml-100k/u.data', sep='\\t', header=None)\n",
    "df_data.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean          3.529860\n",
       "std           1.125674\n",
       "min           1.000000\n",
       "25%           3.000000\n",
       "50%           4.000000\n",
       "75%           4.000000\n",
       "max           5.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader function serves to specify which columns are referring to user, items and ratings as well as the rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sur.Reader(rating_scale=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data_1 = sur.Dataset.load_from_df(\n",
    "    df_data[['user_id', 'item_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9167802882204997\n"
     ]
    }
   ],
   "source": [
    "algo = sur.SVD(random_state=1,\n",
    "               biased=True,  # isolate biases\n",
    "               reg_all=0.2,  # use regularisation (the same for all)\n",
    "               n_epochs=20,  # number of epochs for stochastic gradient descent search\n",
    "               n_factors=100  # number of factors to retain in SVD\n",
    "               )\n",
    "\n",
    "# we have to build a training set from the data\n",
    "trainset_full = data_1.build_full_trainset()\n",
    "# fit the model\n",
    "algo.fit(trainset_full)\n",
    "\n",
    "# we prepare a test set from the training set\n",
    "trainsetfull_build = trainset_full.build_testset()\n",
    "# obtain the predictions\n",
    "predictions_full = algo.test(trainsetfull_build)\n",
    "# evaluate the predictions\n",
    "print(sur.accuracy.rmse(predictions_full, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid=196, iid=242, r_ui=3.0, est=3.911793041493128, details={'was_impossible': False}),\n",
       " Prediction(uid=196, iid=393, r_ui=4.0, est=3.4204953983043573, details={'was_impossible': False}),\n",
       " Prediction(uid=196, iid=381, r_ui=4.0, est=3.5515672295024134, details={'was_impossible': False}),\n",
       " Prediction(uid=196, iid=251, r_ui=3.0, est=4.108294470660626, details={'was_impossible': False}),\n",
       " Prediction(uid=196, iid=655, r_ui=5.0, est=3.7920116316066803, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_full[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = algo.default_prediction()\n",
    "bu = algo.bu\n",
    "bi = algo.bi\n",
    "pu = algo.pu\n",
    "qi = algo.qi\n",
    "puqi = pu.dot(qi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that internally surprise uses other (inner) indices for users and items than in the original data.\n",
    "> The original ones are the raw indices. There are functions to translate between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 196        item: 580        r_ui = 2.00   est = 3.34   {'was_impossible': False}\n",
      "\n",
      "Results agree: 0.0\n"
     ]
    }
   ],
   "source": [
    "# check that we can reconstruct the predictions using the parameters\n",
    "i = 10\n",
    "print(predictions_full[i])\n",
    "print()\n",
    "uid = predictions_full[i].uid\n",
    "iid = predictions_full[i].iid\n",
    "u_inner = trainset_full.to_inner_uid(uid)\n",
    "i_inner = trainset_full.to_inner_iid(iid)\n",
    "\n",
    "pred_calc = mu + bu[u_inner] + bi[i_inner] + puqi[u_inner, i_inner]\n",
    "print('Results agree:', predictions_full[i].est - pred_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9168\n",
      "MAE:  0.7305\n"
     ]
    }
   ],
   "source": [
    "sur.accuracy.rmse(predictions_full)\n",
    "sur.accuracy.mae(predictions_full);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the predictions in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame([(x.r_ui, x.est) for x in predictions_full],\n",
    "                       columns=['Rating', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9167802882205027"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct RMSE\n",
    "np.sqrt(df_pred.apply(lambda x: (x[0]-x[1])**2, axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7305196298517427"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct MAE\n",
    "df_pred.apply(lambda x: abs(x[0]-x[1]), axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between predicted and true ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>0.591973</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Rating  Predicted\n",
       "Rating     1.000000   0.591973\n",
       "Predicted  0.591973   1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>0.576486</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Rating  Predicted\n",
       "Rating     1.000000   0.576486\n",
       "Predicted  0.576486   1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.451749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>0.451749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Rating  Predicted\n",
       "Rating     1.000000   0.451749\n",
       "Predicted  0.451749   1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation, train-test split and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from https://surprise.readthedocs.io/en/stable/FAQ.html?highlight=raw_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "raw_ratings = data_1.raw_ratings\n",
    "np.random.seed(1)\n",
    "# shuffle ratings if you want\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# A = 90% of the data, B = 10% of the data\n",
    "threshold = int(.9 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "print(len(A_raw_ratings))\n",
    "print(len(B_raw_ratings))\n",
    "\n",
    "data_1.raw_ratings = A_raw_ratings  # data is now the set A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x1a224e9650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_1.raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.SVD(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.951627</td>\n",
       "      <td>0.747188</td>\n",
       "      <td>5.788004</td>\n",
       "      <td>0.209199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.940511</td>\n",
       "      <td>0.745558</td>\n",
       "      <td>5.462567</td>\n",
       "      <td>0.126055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940788</td>\n",
       "      <td>0.740873</td>\n",
       "      <td>5.216815</td>\n",
       "      <td>0.123850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938461</td>\n",
       "      <td>0.741327</td>\n",
       "      <td>4.812452</td>\n",
       "      <td>0.229170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.737476</td>\n",
       "      <td>4.945317</td>\n",
       "      <td>0.123679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_rmse  test_mae  fit_time  test_time\n",
       "0   0.951627  0.747188  5.788004   0.209199\n",
       "1   0.940511  0.745558  5.462567   0.126055\n",
       "2   0.940788  0.740873  5.216815   0.123850\n",
       "3   0.938461  0.741327  4.812452   0.229170\n",
       "4   0.937336  0.737476  4.945317   0.123679"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = sur.model_selection.cross_validate(\n",
    "    algo, data_1, measures=['RMSE', 'MAE'], cv=5)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search...\n",
      "Training score    RMSE: 0.8371\n",
      "Test score (rated items)  RMSE: 0.9478\n",
      "Test score (unrated items)    "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5171933412180628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select your best algo with grid search.\n",
    "print('Grid Search...')\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005]}\n",
    "grid_search = sur.model_selection.GridSearchCV(sur.SVD,\n",
    "                                               param_grid,\n",
    "                                               measures=['rmse'],\n",
    "                                               cv=3,\n",
    "                                               refit=True)\n",
    "grid_search.fit(data_1)\n",
    "\n",
    "algo = grid_search.best_estimator['rmse']\n",
    "\n",
    "# retrain on the whole set A\n",
    "trainset = data_1.build_full_trainset()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Compute score on training set\n",
    "trainset_build = trainset.build_testset()\n",
    "predictions_train = algo.test(trainset_build)\n",
    "print('Training score ', end='   ')\n",
    "sur.accuracy.rmse(predictions_train)\n",
    "\n",
    "# Compute score on rated test set\n",
    "testset = data_1.construct_testset(B_raw_ratings)  # testset is now the set B\n",
    "predictions_test = algo.test(testset)\n",
    "print('Test score (rated items) ', end=' ')\n",
    "sur.accuracy.rmse(predictions_test)\n",
    "\n",
    "# Compute score on unrated data\n",
    "# The anti-test set is the part where we did not have any ratings\n",
    "no_ratings = trainset.build_anti_testset()\n",
    "predictions_no_ratings = algo.test(no_ratings)\n",
    "print('Test score (unrated items) ', end='   ')\n",
    "sur.accuracy.rmse(predictions_no_ratings, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 10000 1483867\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset_build), len(testset), len(no_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 538        item: 183        r_ui = 4.00   est = 3.91   {'was_impossible': False}\n",
      "user: 49         item: 294        r_ui = 1.00   est = 1.98   {'was_impossible': False}\n",
      "user: 538        item: 302        r_ui = 3.53   est = 4.05   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "print(predictions_train[0])\n",
    "print(predictions_test[0])\n",
    "print(predictions_no_ratings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean: 3.52949\n"
     ]
    }
   ],
   "source": [
    "# extract model parameters\n",
    "mu = algo.default_prediction()\n",
    "print(f'Training set mean: {mu:.6}')\n",
    "bu = algo.bu\n",
    "bi = algo.bi\n",
    "pu = algo.pu\n",
    "qi = algo.qi\n",
    "puqi = pu.dot(qi.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.28292524012727094"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 538        item: 196        r_ui = 4.00   est = 3.76   {'was_impossible': False}\n",
      "\n",
      "Results agree: 0.0\n"
     ]
    }
   ],
   "source": [
    "# reconstruct predictions\n",
    "i = 10\n",
    "print(predictions_train[i])\n",
    "print()\n",
    "uid = predictions_train[i].uid\n",
    "iid = predictions_train[i].iid\n",
    "u_inner = trainset.to_inner_uid(uid)\n",
    "i_inner = trainset.to_inner_iid(iid)\n",
    "\n",
    "pred_calc = mu + bu[u_inner] + bi[i_inner] + puqi[u_inner, i_inner]\n",
    "print('Results agree:', predictions_train[i].est - pred_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope One\n",
    "\n",
    "Repeat the same steps with the slope one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.SlopeOne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with Means\n",
    "\n",
    "Repeat the same steps with the kNN with means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.KNNWithMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@k and Recall@k\n",
    "\n",
    "Obtain  precision@k and recall@k following the [example](https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-n predictions\n",
    "\n",
    "Obtain the n top-ranked predictions for each user following the [example](https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-get-the-top-n-recommendations-for-each-user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage\n",
    "\n",
    "The coverage is the fraction of all available items which appear at least once in any recommended list of items.\n",
    "Calculate the coverage of the top-ranked recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
