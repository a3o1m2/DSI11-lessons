{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Recommendation Systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Recommendation-Systems\" data-toc-modified-id=\"Recommendation-Systems-1\">Recommendation Systems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1\">Learning Objectives</a></span></li><li><span><a href=\"#Users-and-items\" data-toc-modified-id=\"Users-and-items-1.2\">Users and items</a></span></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-1.3\">Get the data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Create-two-columns-containing-date-and-year-of-the-given--rating\" data-toc-modified-id=\"Create-two-columns-containing-date-and-year-of-the-given--rating-1.3.0.1\">Create two columns containing date and year of the given  rating</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.2\">Solution (double click)</a></span></li><li><span><a href=\"#Describe-the-rating-data\" data-toc-modified-id=\"Describe-the-rating-data-1.3.0.3\">Describe the rating data</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.4\">Solution (double click)</a></span></li><li><span><a href=\"#How-many-ratings-have-been-given-in-each-month-and-year?\" data-toc-modified-id=\"How-many-ratings-have-been-given-in-each-month-and-year?-1.3.0.5\">How many ratings have been given in each month and year?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.6\">Solution (double click)</a></span></li><li><span><a href=\"#How-many-different-films-have-been-rated-by-how-many-different-users?\" data-toc-modified-id=\"How-many-different-films-have-been-rated-by-how-many-different-users?-1.3.0.7\">How many different films have been rated by how many different users?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.8\">Solution (double click)</a></span></li><li><span><a href=\"#Which-rating-scale-has-been-used?-How-many-times-has-each-rating-been-given?\" data-toc-modified-id=\"Which-rating-scale-has-been-used?-How-many-times-has-each-rating-been-given?-1.3.0.9\">Which rating scale has been used? How many times has each rating been given?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.10\">Solution (double click)</a></span></li><li><span><a href=\"#How-many-ratings-were-given-by-each-user?\" data-toc-modified-id=\"How-many-ratings-were-given-by-each-user?-1.3.0.11\">How many ratings were given by each user?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.12\">Solution (double click)</a></span></li><li><span><a href=\"#How-many-ratings-were-received-by-each-film?\" data-toc-modified-id=\"How-many-ratings-were-received-by-each-film?-1.3.0.13\">How many ratings were received by each film?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.3.0.14\">Solution (double click)</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.4\">Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#MAE-and-MSE\" data-toc-modified-id=\"MAE-and-MSE-1.4.1\">MAE and MSE</a></span></li><li><span><a href=\"#Correlations\" data-toc-modified-id=\"Correlations-1.4.2\">Correlations</a></span></li><li><span><a href=\"#Precision@k-and-recall@k\" data-toc-modified-id=\"Precision@k-and-recall@k-1.4.3\">Precision@k and recall@k</a></span></li></ul></li><li><span><a href=\"#Baseline-prediction\" data-toc-modified-id=\"Baseline-prediction-1.5\">Baseline prediction</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-1.5.0.1\">Example</a></span></li><li><span><a href=\"#Determine-the-baseline-estimate-for-each-user-item-pair\" data-toc-modified-id=\"Determine-the-baseline-estimate-for-each-user-item-pair-1.5.0.2\">Determine the baseline estimate for each user-item pair</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.0.3\">Solution (double click)</a></span></li><li><span><a href=\"#Is-there-any-problem-with-determining-the-baseline-like-this?\" data-toc-modified-id=\"Is-there-any-problem-with-determining-the-baseline-like-this?-1.5.0.4\">Is there any problem with determining the baseline like this?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.0.5\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#User-item-matrix\" data-toc-modified-id=\"User-item-matrix-1.5.1\">User-item matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.1.1\">Solution (double click)</a></span></li><li><span><a href=\"#Evaluate-the-baseline-prediction-against-the-true-values\" data-toc-modified-id=\"Evaluate-the-baseline-prediction-against-the-true-values-1.5.1.2\">Evaluate the baseline prediction against the true values</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.1.3\">Solution (double click)</a></span></li><li><span><a href=\"#Form-recommendation-lists-by-proposing-to-each-user-the-five-films-ranked-highest-according-to-their-baseline.\" data-toc-modified-id=\"Form-recommendation-lists-by-proposing-to-each-user-the-five-films-ranked-highest-according-to-their-baseline.-1.5.1.4\">Form recommendation lists by proposing to each user the five films ranked highest according to their baseline.</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.1.5\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Slope-one-predictor\" data-toc-modified-id=\"Slope-one-predictor-1.5.2\">Slope-one predictor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-1.5.2.1\">Example</a></span></li><li><span><a href=\"#According-to-this-model,-how-would-user-8-rate-film-1?\" data-toc-modified-id=\"According-to-this-model,-how-would-user-8-rate-film-1?-1.5.2.2\">According to this model, how would user 8 rate film 1?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.5.2.3\">Solution (double click)</a></span></li></ul></li></ul></li><li><span><a href=\"#Measuring-(dis-)similarity-between-ratings\" data-toc-modified-id=\"Measuring-(dis-)similarity-between-ratings-1.6\">Measuring (dis-)similarity between ratings</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Determine-the-pairwise-distances-between-items\" data-toc-modified-id=\"Determine-the-pairwise-distances-between-items-1.6.0.1\">Determine the pairwise distances between items</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.6.0.2\">Solution (double click)</a></span></li><li><span><a href=\"#Based-on-the-user-cosine-similarity,-determine-who-are-the-three-closest-users-to-user-8\" data-toc-modified-id=\"Based-on-the-user-cosine-similarity,-determine-who-are-the-three-closest-users-to-user-8-1.6.0.3\">Based on the user-cosine similarity, determine who are the three closest users to user 8</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.6.0.4\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#KNN-with-means\" data-toc-modified-id=\"KNN-with-means-1.6.1\">KNN with means</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-1.6.1.1\">Example</a></span></li><li><span><a href=\"#What-would-we-predict-for-the-rating-given-by-user-8-for-film-1-with-this-model-with-$k=3$-and-using-cosine-similarity?\" data-toc-modified-id=\"What-would-we-predict-for-the-rating-given-by-user-8-for-film-1-with-this-model-with-$k=3$-and-using-cosine-similarity?-1.6.1.2\">What would we predict for the rating given by user 8 for film 1 with this model with $k=3$ and using cosine similarity?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.6.1.3\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Singular-Value-decomposition\" data-toc-modified-id=\"Singular-Value-decomposition-1.6.2\">Singular Value decomposition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-the-predictions-using-MSE,-MAE,-Pearson-and-Spearman-correlations\" data-toc-modified-id=\"Evaluate-the-predictions-using-MSE,-MAE,-Pearson-and-Spearman-correlations-1.6.2.1\">Evaluate the predictions using MSE, MAE, Pearson and Spearman correlations</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-1.6.2.2\">Solution (double click)</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-1.7\">Conclusions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Learning Objectives\n",
    "\n",
    "- Understand what data is needed to form recommendations\n",
    "- Learn how to evaluate a recommendation model\n",
    "- Meet the main types of recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to recommend items to users, we face a few fundamental problems:\n",
    "\n",
    "1. Data Sparsity\n",
    "    - There are lots of products to recommend to many users. \n",
    "    - It is unlikely that a user will ever try out a large fraction of products.\n",
    "    - A few items are demanded by many users, but many only by a few.\n",
    "   \n",
    "- Cold Start\n",
    "    - We need to be able to give recommendations to users about which we only have scarse data (if at all).\n",
    "    \n",
    "- Accurate, but also diverse predictions\n",
    "    - We want to give useful recommendations in the sense that they match the user's preferences, but also that the recommendation contains some novelty for the user. \n",
    "\n",
    "- Evaluation\n",
    "    - Evaluation is difficult and might differ from algorithm to algorithm.\n",
    "\n",
    "- Scalability\n",
    "    - We need to be able to give recommendations on the spot even though there might be millions of users and items which we have to analyze carefully.\n",
    "\n",
    "- User interface\n",
    "    - Users want to know why they get particular recommendations.\n",
    "\n",
    "- Vulnerability to attacks\n",
    "    - We do not want our recommendation system to be abused for promoting or inhibiting particular items.\n",
    " \n",
    "- Temporal resolution\n",
    "    - Tastes and preferences do not remain the same over time. The algorithms that we will see neglect any kind of dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users and items\n",
    "\n",
    "In general we speak of users and items.\n",
    "\n",
    "> **Users:** indicate preferences for products through explicit/implicit ratings\n",
    "\n",
    "> **Items:** products which should be recommended and which have received ratings\n",
    "\n",
    "In most cases we are going to predict a certain rating for each possible pair of user and item. If the user already gave some rating we can compare it to our prediction:\n",
    "\n",
    "- True rating: $r_{ui}$\n",
    "- Predicted rating: $\\hat{r}_{ui}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need some data. Download the Movielens 100K dataset from [here](https://grouplens.org/datasets/movielens/). \n",
    "It is a very famous dataset about movie ratings. Adjust the paths to the files as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY & USAGE LICENSE\n",
      "=============================================\n",
      "\n",
      "MovieLens data sets were collected by the GroupLens Research Project\n",
      "at the University of Minnesota.\n",
      " \n",
      "This data set consists of:\n",
      "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
      "\t* Each user has rated at least 20 movies. \n",
      "        * Simple demographic info for the users (age, gender, occupation, zip)\n",
      "\n",
      "The data was collected through the MovieLens web site\n",
      "(movielens.umn.edu) during the seven-month period from September 19th, \n",
      "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
      "who had less than 20 ratings or did not have complete demographic\n",
      "information were removed from this data set. Detailed descriptions of\n",
      "the data file can be found at the end of this file.\n",
      "\n",
      "Neither the University of Minnesota nor any of the researchers\n",
      "involved can guarantee the correctness of the data, its suitability\n",
      "for any particular purpose, or the validity of results based on the\n",
      "use of the data set.  The data set may be used for any research\n",
      "purposes under the following conditions:\n",
      "\n",
      "     * The user may not state or imply any endorsement from the\n",
      "       University of Minnesota or the GroupLens Research Group.\n",
      "\n",
      "     * The user must acknowledge the use of the data set in\n",
      "       publications resulting from the use of the data set\n",
      "       (see below for citation information).\n",
      "\n",
      "     * The user may not redistribute the data without separate\n",
      "       permission.\n",
      "\n",
      "     * The user may not use this information for any commercial or\n",
      "       revenue-bearing purposes without first obtaining permission\n",
      "       from a faculty member of the GroupLens Research Project at the\n",
      "       University of Minnesota.\n",
      "\n",
      "If you have any further questions or comments, please contact GroupLens\n",
      "<grouplens-info@cs.umn.edu>. \n",
      "\n",
      "CITATION\n",
      "==============================================\n",
      "\n",
      "To acknowledge use of the dataset in publications, please cite the \n",
      "following paper:\n",
      "\n",
      "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:\n",
      "History and Context. ACM Transactions on Interactive Intelligent\n",
      "Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.\n",
      "DOI=http://dx.doi.org/10.1145/2827872\n",
      "\n",
      "\n",
      "ACKNOWLEDGEMENTS\n",
      "==============================================\n",
      "\n",
      "Thanks to Al Borchers for cleaning up this data and writing the\n",
      "accompanying scripts.\n",
      "\n",
      "PUBLISHED WORK THAT HAS USED THIS DATASET\n",
      "==============================================\n",
      "\n",
      "Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic\n",
      "Framework for Performing Collaborative Filtering. Proceedings of the\n",
      "1999 Conference on Research and Development in Information\n",
      "Retrieval. Aug. 1999.\n",
      "\n",
      "FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT\n",
      "==============================================\n",
      "\n",
      "The GroupLens Research Project is a research group in the Department\n",
      "of Computer Science and Engineering at the University of Minnesota.\n",
      "Members of the GroupLens Research Project are involved in many\n",
      "research projects related to the fields of information filtering,\n",
      "collaborative filtering, and recommender systems. The project is lead\n",
      "by professors John Riedl and Joseph Konstan. The project began to\n",
      "explore automated collaborative filtering in 1992, but is most well\n",
      "known for its world wide trial of an automated collaborative filtering\n",
      "system for Usenet news in 1996.  The technology developed in the\n",
      "Usenet trial formed the base for the formation of Net Perceptions,\n",
      "Inc., which was founded by members of GroupLens Research. Since then\n",
      "the project has expanded its scope to research overall information\n",
      "filtering solutions, integrating in content-based methods as well as\n",
      "improving current collaborative filtering technology.\n",
      "\n",
      "Further information on the GroupLens Research project, including\n",
      "research publications, can be found at the following web site:\n",
      "        \n",
      "        http://www.grouplens.org/\n",
      "\n",
      "GroupLens Research currently operates a movie recommender based on\n",
      "collaborative filtering:\n",
      "\n",
      "        http://www.movielens.org/\n",
      "\n",
      "DETAILED DESCRIPTIONS OF DATA FILES\n",
      "==============================================\n",
      "\n",
      "Here are brief descriptions of the data.\n",
      "\n",
      "ml-data.tar.gz   -- Compressed tar file.  To rebuild the u data files do this:\n",
      "                gunzip ml-data.tar.gz\n",
      "                tar xvf ml-data.tar\n",
      "                mku.sh\n",
      "\n",
      "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
      "              Each user has rated at least 20 movies.  Users and items are\n",
      "              numbered consecutively from 1.  The data is randomly\n",
      "              ordered. This is a tab separated list of \n",
      "\t         user id | item id | rating | timestamp. \n",
      "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
      "\n",
      "u.info     -- The number of users, items, and ratings in the u data set.\n",
      "\n",
      "u.item     -- Information about the items (movies); this is a tab separated\n",
      "              list of\n",
      "              movie id | movie title | release date | video release date |\n",
      "              IMDb URL | unknown | Action | Adventure | Animation |\n",
      "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
      "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
      "              Thriller | War | Western |\n",
      "              The last 19 fields are the genres, a 1 indicates the movie\n",
      "              is of that genre, a 0 indicates it is not; movies can be in\n",
      "              several genres at once.\n",
      "              The movie ids are the ones used in the u.data data set.\n",
      "\n",
      "u.genre    -- A list of the genres.\n",
      "\n",
      "u.user     -- Demographic information about the users; this is a tab\n",
      "              separated list of\n",
      "              user id | age | gender | occupation | zip code\n",
      "              The user ids are the ones used in the u.data data set.\n",
      "\n",
      "u.occupation -- A list of the occupations.\n",
      "\n",
      "u1.base    -- The data sets u1.base and u1.test through u5.base and u5.test\n",
      "u1.test       are 80%/20% splits of the u data into training and test data.\n",
      "u2.base       Each of u1, ..., u5 have disjoint test sets; this if for\n",
      "u2.test       5 fold cross validation (where you repeat your experiment\n",
      "u3.base       with each training and test set and average the results).\n",
      "u3.test       These data sets can be generated from u.data by mku.sh.\n",
      "u4.base\n",
      "u4.test\n",
      "u5.base\n",
      "u5.test\n",
      "\n",
      "ua.base    -- The data sets ua.base, ua.test, ub.base, and ub.test\n",
      "ua.test       split the u data into a training set and a test set with\n",
      "ub.base       exactly 10 ratings per user in the test set.  The sets\n",
      "ub.test       ua.test and ub.test are disjoint.  These data sets can\n",
      "              be generated from u.data by mku.sh.\n",
      "\n",
      "allbut.pl  -- The script that generates training and test sets where\n",
      "              all but n of a users ratings are in the training data.\n",
      "\n",
      "mku.sh     -- A shell script to generate all the u data sets from u.data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/crahmede/.surprise_data/ml-100k/ml-100k/README', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/crahmede/.surprise_data/ml-100k/ml-100k/u.data', \n",
    "                 header=None, \n",
    "                 sep='\\t', \n",
    "                 names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp                date\n",
       "0      196      242       3  881250949 1997-12-04 15:55:49\n",
       "1      186      302       3  891717742 1998-04-04 19:22:22\n",
       "2       22      377       1  878887116 1997-11-07 07:18:36\n",
       "3      244       51       2  880606923 1997-11-27 05:02:03\n",
       "4      166      346       1  886397596 1998-02-02 05:33:16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.min(), df.item_id.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work better with python indices, we adjust the id labels so that they start at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['user_id'] - 1\n",
    "df['item_id'] = df['item_id'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create two columns containing date and year of the given  rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df['year'] = df.date.dt.year\n",
    "df['month'] = df.date.dt.month\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the rating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df.describe()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many ratings have been given in each month and year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df.groupby(['year', 'month'], sort=True)[['date']].count().plot(kind='barh')\n",
    "plt.show()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many different films have been rated by how many different users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "print(df.user_id.unique().shape)\n",
    "print(df.item_id.unique().shape)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which rating scale has been used? How many times has each rating been given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "print(np.sort(df.rating.unique()))\n",
    "df.rating.value_counts(sort=False, normalize=True).plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('rating')\n",
    "plt.show()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many ratings were given by each user?\n",
    "\n",
    "- Determine the user with the highest number of ratings.\n",
    "- Plot a histogram with the rating count per user distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "print(df.groupby('user_id')['rating'].count().argmax())\n",
    "print(df.groupby('user_id')['rating'].count().max())\n",
    "df.groupby('user_id')['rating'].count().hist()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many ratings were received by each film?\n",
    "\n",
    "- Determine the film with the highest number of ratings.\n",
    "- Plot a histogram with the rating count per film distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "print(df.groupby('item_id')['rating'].count().argmax())\n",
    "print(df.groupby('item_id')['rating'].count().max())\n",
    "df.groupby('item_id')['rating'].count().hist()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE and MSE\n",
    "\n",
    "We can compare all existing ratings to our prediction for example using the root mean squared error (RMSE) or the mean absolute error (MAE):\n",
    "\n",
    "    \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm MAE} &=& \\frac{1}{|R|}\\sum_{(u,i)\\in R}|r_{ui}-{\\tilde r}_{ui}|\\\\\n",
    "{\\rm RMSE} &=& \\left(\\frac{1}{|R|}\\sum_{(u,i)\\in R}(r_{ui}-{\\tilde r}_{ui})^2\\right)^{1/2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Here, $R$ stands for the set of all user-item pairs. $|\\cdot|$ indicates the cardinality of the set (here the number of user-item pairs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "Alternatively one can use correlations between true and predicted values for model evaluation, e.g.\n",
    "\n",
    "- the Pearson correlation\n",
    "- the Spearman rank correlation\n",
    "- Kendall's tau\n",
    "\n",
    "You can call these three for example with panda's `.corr()` function by setting the `method` argument.\n",
    "    \n",
    "The above scores are alright to obtain a model assessment if we have explicit ratings, but in the case of implicit ratings we might only be able to rank the items. In general, we would like to recommend the top-ranked items, but we have to evaluate if the top-ranked ones are really the ones relevant to the user, or if for some irrelevant items we predicted higher ratings.\n",
    "In that regard, we can use the usual classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@k and recall@k\n",
    "\n",
    "Often users will not really care about all the rating predictions we are making, but instead they will have a major interest only in a few top-ranked items, let's say the $k$ top-ranked items. So it is appropriate to take only these $k$ ratings into account. We then ask how many of these $k$ items are relevant to the user. The relevance is in general difficult to measure, but we can for example ask how many out of the $k$ top-ranked items have a score beyond a certain threshold.\n",
    "\n",
    "We can then define the so-called precision@k and recall@k for $k$ recommended items:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm precision@k} &=& \\frac{\\rm  Recommended\\ items\\ that\\ are\\ relevant}{\\rm Recommended\\ items}\\\\\n",
    "\\\\\n",
    "{\\rm recall@k} &=& \n",
    "\\frac{\\rm  Recommended\\ items\\ that\\ are\\ relevant}{\\rm Relevant\\ items}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Out of these scores we can define an F1@k score in the usual way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction\n",
    "\n",
    "As we are predicting ratings $\\hat{r}_{ui}$ of user $u$ on item $i$, the baseline should be the mean of all ratings, $\\mu$.\n",
    "\n",
    "As we will always be considering a specific user or a specific item, we can determine how much each user's or item's ratings are above the average. \n",
    "\n",
    "Therefore we add a bias term $b_u$ for each user and $b_i$ for each item, so that our baseline is\n",
    "\n",
    "$$\n",
    "{\\rm baseline}_{ui} = \\mu + b_u + b_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let's say for example that the average rating is $\\mu=3.52$. Our user is very enthusiastic and on average evaluates items better than the average user by $b_u=0.3$. The item is very popular receiving above average ratings with $b_i=0.5$. So we would have a baseline prediction of $4.32$.\n",
    "\n",
    "We will look at various models which make more accurate predictions based on either similarity or content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the baseline estimate for each user-item pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "mu = df.rating.mean()\n",
    "print(mu)\n",
    "bu = df.groupby('user_id')[['rating']].mean() - mu\n",
    "print(bu.head())\n",
    "bi = df.groupby('item_id')[['rating']].mean() - mu\n",
    "print(bi.head())\n",
    "baseline = mu + bu.values.reshape(-1, 1) + bi.values.reshape(1, -1)\n",
    "print(baseline)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there any problem with determining the baseline like this? \n",
    "\n",
    "- Check the range of the baseline ratings.\n",
    "- How would you adjust that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "print(baseline.argmax(), baseline.max())\n",
    "print(baseline.min())\n",
    "print(int(baseline.argmax() / baseline.shape[1]))\n",
    "print(baseline.argmax() % baseline.shape[1])\n",
    "print(baseline[848, 813])\n",
    "baseline[baseline>5] = 5\n",
    "baseline[baseline<1] = 1\n",
    "print(baseline.max(), baseline.min())\n",
    "plt.hist(baseline.ravel())\n",
    "plt.show()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-item matrix\n",
    "\n",
    "We can construct the user-item matrix by reading all user-item-rating triples into a scipy-sparse matrix. Recall how that would work for any matrix. How would you apply this procedure to the given ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "A = sparse.csr_matrix((data, (row, col)), shape=(3, 3))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df.user_id.unique().shape\n",
    "row = df.user_id\n",
    "col = df.item_id\n",
    "data = df.rating\n",
    "rui = sparse.csr_matrix((data, (row, col)), shape=(df.user_id.unique().shape[0], df.item_id.unique().shape[0]))\n",
    "rui\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the baseline prediction against the true values\n",
    "\n",
    "Evaluate using \n",
    "- the MSE\n",
    "- the MAE\n",
    "- the Pearson correlation\n",
    "- the Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rui_flat = rui.toarray().ravel()\n",
    "residuals = (rui_flat-baseline.ravel())[rui_flat>0]\n",
    "print(np.mean(np.square(residuals)))\n",
    "print(np.mean(np.abs(residuals)))\n",
    "print(pearsonr(rui_flat[rui_flat>0], baseline.ravel()[rui_flat>0])[0])\n",
    "print(spearmanr(rui_flat[rui_flat>0], baseline.ravel()[rui_flat>0])[0])\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form recommendation lists by proposing to each user the five films ranked highest according to their baseline. \n",
    "\n",
    "Calculate the fraction of all the films which are ever proposed in any of these lists. This fraction is called the **coverage**. Most recommender systems would make sure to increase this coverage. Are you proposing films which have been rated very frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "baseline_recommendation = []\n",
    "for i in range(len(baseline)):\n",
    "    baseline_recommendation.append(sorted(list(zip(range((baseline.shape[1])), baseline[i])), key=lambda x: x[1], reverse=True)[:5])\n",
    "baseline_recommendation = np.array(baseline_recommendation)\n",
    "print(baseline_recommendation[0:, :, 0])\n",
    "baseline_recommendation_unique = np.unique(baseline_recommendation[0:, :, 0].ravel()).astype(int)\n",
    "print(baseline_recommendation_unique)\n",
    "print(df.groupby('item_id')['rating'].count().loc[baseline_recommendation_unique])\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope-one predictor\n",
    "\n",
    "\n",
    "This scheme makes use of both the information from other users who rated the same item and from the other items rated by the same user to predict a rating:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_u + \\frac{1}{|R_i(u)|}\\sum_{j \\in R_i(u)}{\\rm dev}(i,j)\n",
    "$$\n",
    "\n",
    "where $R_i(u)$ is the set of items rated by user $u$\n",
    "and the average difference between the ratings of item $i$ and $j$ is\n",
    "\n",
    "$$\n",
    "{\\rm dev}(i,j) = \\frac{1}{|U_{ij}|}\\sum_{u\\in U_{ij}}(r_{ui}-r_{uj})\n",
    "$$\n",
    "\n",
    "and $U_{ij}$ is the set of all users that have rated both items $i$ and $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Consider the following example of ratings\n",
    "\n",
    "|   -     | item 1 | item 2|\n",
    "| ------- |:------:| -----:|\n",
    "| user 1  | 2      | 1.8   |\n",
    "| user 2  | 1      |  ?    |\n",
    "\n",
    "Then we have\n",
    "\n",
    "$\\mu_{\\rm user 2}=1$, $|U_{12}|=1$, $r_{11}=2$, $r_{12}=1.8$ and\n",
    "\n",
    "$$\n",
    "r_{22} = 1+\\frac{1}{1}(1.8-2) = 0.8\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to this model, how would user 8 rate film 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2727272727272725"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_user_8 = df[df.user_id==8].rating.mean()\n",
    "mu_user_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([297, 690, 520, 486, 285,   5, 478, 339, 526, 506, 275, 614, 689,\n",
       "       293, 482, 401, 370, 241, 200,  49,   6, 384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rated_by_user_8 = df[df.user_id==8]['item_id'].values\n",
    "rated_by_user_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rui_np = rui.toarray()\n",
    "def slope_one_predictor(user_predict, item_predict, rui_np):\n",
    "    rated_by_user = np.arange(rui_np.shape[1])[rui_np[user_predict]>0]\n",
    "    user_mean = (rui_np[user_predict][rui_np[user_predict]>0]).mean()\n",
    "    deviance = 0\n",
    "    for item in rated_by_user:\n",
    "        deviance_current = rui_np[((rui_np[:, item_predict]>0) & (rui_np[:, item]>0))]\n",
    "        deviance_current = (deviance_current[:, item_predict] - deviance_current[:, item]).sum() / deviance_current.shape[0]\n",
    "        deviance += deviance_current\n",
    "    deviance = deviance / len(rated_by_user)\n",
    "    deviance += user_mean\n",
    "    return deviance\n",
    "slope_one_predictor(8, 1, rui.toarray())\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring (dis-)similarity between ratings\n",
    "\n",
    "We use similarity measures to infer from past ratings what to recommend in the future. This can be \n",
    "\n",
    "- **item based:** a user who already rated some items positively would like to have similar items recommended in the future\n",
    "- **user based:** users who agree on their item ratings will do so also in the future\n",
    "\n",
    "We can use similarity measures we have already seen frequently:\n",
    "\n",
    "- **Correlation similarity:** \n",
    "\n",
    "$$\n",
    "{\\rm sim}_{\\rm corr}(u,v) = \\frac{(u-\\bar{u})\\cdot (v-\\bar{v})}{\\|u\\|\\|v\\|}\n",
    "$$\n",
    "\n",
    "- **Cosine similarity**\n",
    "\n",
    "$$\n",
    "{\\rm sim}_{\\cos}(u,v) = \\frac{u\\cdot v}{\\|u\\|\\|v\\|}\n",
    "$$\n",
    "\n",
    "- **Mean squared difference**\n",
    "\n",
    "$$\n",
    "{\\rm msd}(u,v) = \\frac{1}{|I_{uv}|}\\sum_{i\\in I_{uv}}(r_{ui}-r_{vi})^2\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "{\\rm msd\\_sim}(u,v) = \\frac{1}{{\\rm msd}(u,v)+1} \n",
    "$$\n",
    "\n",
    "and similarly for item pairs $i,j$. $|I_{uv}|$ is the number of items rated by both users $u$ and $v$.\n",
    "\n",
    "There exist many more based on the usual distance metrics, but also many specialized for recommendation system requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity, pairwise_distances, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1682 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.user_id\n",
    "col = df.item_id\n",
    "data = df.rating\n",
    "rui = sparse.csr_matrix((data, (row, col)), shape=(df.user_id.unique().shape[0], df.item_id.unique().shape[0]))\n",
    "rui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.16693098, 0.04745954, 0.06435782, 0.37847518,\n",
       "       0.43023944, 0.4403668 , 0.31907211, 0.07813837, 0.37654381])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity_cos = cosine_similarity(rui)\n",
    "user_similarity_cos[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.83306902, 0.95254046, 0.93564218, 0.62152482,\n",
       "       0.56976056, 0.5596332 , 0.68092789, 0.92186163, 0.62345619])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_distance_cos = cosine_distances(rui)\n",
    "user_distance_cos[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.89367807, 1.01642393, 0.97828412, 0.69740786,\n",
       "       0.65402415, 0.67998316, 0.7237923 , 0.96118849, 0.71150891])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_distance_corr = pairwise_distances(rui.toarray(), metric='correlation')\n",
    "user_distance_corr[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msd(x, y):\n",
    "    \"\"\"\n",
    "    check for items rated by both users,\n",
    "    then calculate Euclidean distance between users\n",
    "    \"\"\"\n",
    "    condition = (x!=0) & (y!=0)\n",
    "    try:\n",
    "        x_c = x[condition].reshape(1, -1)\n",
    "        y_c = y[condition].reshape(1, -1)\n",
    "        e = (euclidean_distances(x_c, y_c)[0, 0])**2 / condition.sum()\n",
    "        return 1 - 1 / (1 + e)\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.6       , 0.76470588, 0.73076923, 0.64912281,\n",
       "       0.62790698, 0.66512702, 0.46031746, 0.5       , 0.53892216])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_distance_msd = pairwise_distances(rui.toarray(), metric=msd)\n",
    "user_distance_msd[0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the pairwise distances between items\n",
    "\n",
    "Hint: Calculate the same as above for the transposed user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "item_distance_cos = cosine_distances(rui.T)\n",
    "print(item_distance_cos[0, :10])\n",
    "item_distance_corr = pairwise_distances(rui.T.toarray(), metric='correlation')\n",
    "print(item_distance_corr[0, :10])\n",
    "item_distance_msd = pairwise_distances(rui.T.toarray(), metric=msd)\n",
    "print(item_distance_msd[0, :10])\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the user-cosine similarity, determine who are the three closest users to user 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "sorted(zip(np.arange(user_similarity_cos.shape[0]), user_similarity_cos[8]), \n",
    "       key=lambda x: x[1], reverse=True)[:4]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with means\n",
    "\n",
    "In the kNN with means model, we look at either the $k$ most similar users or items and predict based on\n",
    "\n",
    "- user similarity:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_u+\\frac{\\sum_{v \\in N_i^k(u)}{\\rm sim}(u,v)(r_{vi}-\\mu_v)}{\\sum_{v \\in N_i^k(u)}{\\rm sim}(u,v)}\n",
    "$$\n",
    "\n",
    "- item similarity:\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_i+\\frac{\\sum_{j \\in N_u^k(i)}{\\rm sim}(i,j)(r_{uj}-\\mu_j)}{\\sum_{j \\in N_u^k(i)}{\\rm sim}(i,j)}\n",
    "$$\n",
    "\n",
    "Here $N_i^k(u)$ denotes the $k$ most similar users to user $u$ who rated item $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "As an example, let's take user similarity (e.g. correlation similarity) and we consider the two nearest neighbors of user $1$.\n",
    "\n",
    "Let's say we have \n",
    "\n",
    "$\\mu_1 = 3$, $\\mu_2 = 2$, $\\mu_3 = 4$,\n",
    "${\\rm sim}(1,2)=0.8$, ${\\rm sim}(1,3)=0.5$, \n",
    "\n",
    "and want to predict for item $1$ with \n",
    "\n",
    "$r_{21}=3.2$ and $r_{31}=3.8$.\n",
    "\n",
    "Then we obtain\n",
    "\n",
    "$$\n",
    "r_{11} = 3 +\\frac{0.8(3.2-2)+0.5(3.8-4)}{0.8+0.5}\n",
    "= 3+\\frac{0.8\\cdot1.2+0.5\\cdot(-0.2)}{1.3}\n",
    "\\approx 3.66\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What would we predict for the rating given by user 8 for film 1 with this model with $k=3$ and using cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "def k_with_means(user_predict, item_predict, rui_np, similarities, k=3):\n",
    "    user_mean = (rui_np[user_predict][rui_np[user_predict]>0]).mean()   \n",
    "    users_who_rated = np.arange(rui_np.shape[0])[rui_np[:, item_predict]>0]\n",
    "    user_similarities = similarities[user_predict][rui_np[:, item_predict]>0]\n",
    "    most_similar_users = sorted(list(zip(users_who_rated, user_similarities)), \n",
    "                            key=lambda x: x[1], reverse=True)[:k]\n",
    "    similarities = np.array([sim for user, sim in most_similar_users ])\n",
    "    user_deviations = []\n",
    "    for user, sim in most_similar_users:\n",
    "        r_vi = rui_np[user, item_predict]\n",
    "        user_mean_v = (rui_np[user][rui_np[user]>0]).mean()\n",
    "        user_deviations.append(r_vi - user_mean_v)        \n",
    "    user_deviations = np.array(user_deviations)\n",
    "    prediction = user_mean + np.dot(similarities, user_deviations)/similarities.sum()    \n",
    "    return prediction\n",
    "k_with_means(10, 1, rui_np, user_similarity_cos)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value decomposition\n",
    "\n",
    "Singular value decomposition is related to principal component analysis. It makes use of the spectral theorem that every matrix $X$ of shape $n\\times p$ can be written as a dot product of three matrices\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U$ is of shape $n\\times n$, $V$ is of shape $p\\times p$ and $\\Sigma$ of shape $n\\times p$ is a diagonal matrix with the so-called singular values on its diagonal (which are simply the square roots of the eigenvalues of $X^T X$). Both $U$ and $V$ are orthogonal matrices (which means their inverse is their transpose).\n",
    "\n",
    "In the same way as for principal component analysis, we can reduce the dimensionality by restricting to the components with the $K$ largest singular values.\n",
    "\n",
    "This is what we are going to do with the user-item matrix. As it is very sparse anyway, we can expect that we won't loose too much information in this way and that we will somehow extract the main tastes and item attributes in this way.\n",
    "\n",
    "That is whereas the rating matrix can be exactly written as\n",
    "\n",
    "$$\n",
    "R = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "we can approximate it by choosing $\\Sigma$ of shape $K\\times K$ where $K<{\\rm min}(n,p)$ as\n",
    "\n",
    "$$\n",
    "\\hat{R} = U\\Sigma_K V^T\n",
    "$$\n",
    "\n",
    "Not restricting $K$ will lead to perfect predictions on the training set.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.21564507e+01,  1.46291293e+00, -1.33595403e+00, ...,\n",
       "         2.40035622e-01,  2.19761720e-01,  6.43634360e-01],\n",
       "       [ 8.98235172e+00, -1.14157452e+01,  1.14540401e+01, ...,\n",
       "        -7.94876790e-01, -1.37208950e+00, -4.64055187e-01],\n",
       "       [ 3.62469291e+00, -6.27232708e+00,  5.08928630e+00, ...,\n",
       "         1.35355500e-01,  8.76252607e-02, -2.02693442e-01],\n",
       "       ...,\n",
       "       [ 4.76921060e+00, -6.12612044e+00,  1.34309272e+00, ...,\n",
       "         4.25552191e-02, -7.35491683e-03, -2.42807964e-01],\n",
       "       [ 1.53951858e+01,  1.98222287e+00,  4.98592430e+00, ...,\n",
       "        -9.33711489e-01,  3.43300619e-01,  3.91089695e-01],\n",
       "       [ 2.70617041e+01, -2.67536312e+00, -1.27540348e+01, ...,\n",
       "        -1.68608473e+00, -1.17968492e+00, -1.45224707e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=400)\n",
    "rui_transformed = svd.fit_transform(rui)\n",
    "rui_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.95859054e+00,  2.94829939e+00,  3.70979041e+00, ...,\n",
       "         8.52650385e-03, -9.93049309e-03,  4.32643872e-02],\n",
       "       [ 3.93374988e+00,  3.44893657e-01,  4.62819467e-01, ...,\n",
       "        -1.93306899e-02, -4.07867629e-02,  4.41300631e-04],\n",
       "       [-6.42163525e-03, -2.12489040e-01, -5.68957579e-02, ...,\n",
       "         5.90014380e-02,  2.99654415e-02, -4.08978613e-03],\n",
       "       ...,\n",
       "       [ 4.96101511e+00, -3.08578350e-01, -1.90203613e-01, ...,\n",
       "         4.46497012e-04,  4.13624175e-02, -1.52370097e-02],\n",
       "       [-1.45304123e-01,  3.72512344e-01,  7.23479756e-02, ...,\n",
       "        -4.98603834e-02, -2.90245048e-03,  7.86873694e-03],\n",
       "       [-2.81577568e-02,  4.58021739e+00, -8.32127789e-03, ...,\n",
       "         4.11476481e-02,  9.88231466e-02, -2.71771790e-02]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rui_predicted = svd.inverse_transform(rui_transformed)\n",
    "rui_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rui_predicted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the predictions using MSE, MAE, Pearson and Spearman correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "residuals_svd = (rui_flat-rui_predicted.ravel())[rui_flat>0]\n",
    "print(np.mean(np.square(residuals_svd)))\n",
    "print(np.mean(np.abs(residuals_svd)))\n",
    "print(pearsonr(rui_flat[rui_flat>0], rui_predicted.ravel()[rui_flat>0])[0])\n",
    "print(spearmanr(rui_flat[rui_flat>0], rui_predicted.ravel()[rui_flat>0])[0])\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "There are many more recommendation system models, but with these simple models one can already reach quite far. What is still lacking is to establish an efficient implementation that allows us to cross-validate our predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
