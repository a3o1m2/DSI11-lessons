{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Visualizing and tuning CARTs\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the admissions data from earlier in the course, build CARTs, look at how they work visually, and compare their performance to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install and load the packages required to visually show decision tree branching\n",
    "\n",
    "You will need to first:\n",
    "\n",
    "1. Install `graphviz` with homebrew (on OSX). The command will be `brew install graphviz`\n",
    "- Install `pydotplus` with `conda install -c anaconda pydotplus`\n",
    "- Load the packages as shown below (you may need to restart the kernel after the installations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS:\n",
    "# conda install -c anaconda pydotplus\n",
    "# brew install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load in admissions data and other python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit = pd.read_csv('../../../../resource-datasets/admissions/admissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create regression and classification X, y data\n",
    "\n",
    "The regression data will be:\n",
    "\n",
    "    Xr = [admit, gre, prestige]\n",
    "    yr = gpa\n",
    "    \n",
    "The classification data will be:\n",
    "\n",
    "    Xc = [gre, gpa, prestige]\n",
    "    yc = admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross-validate linear regression and logistic regression on the data\n",
    "\n",
    "Fit a linear regression for the regression problem and a logistic regression for the classification problem. Cross-validate the R2 and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building regression trees\n",
    "\n",
    "With `DecisionTreeRegressor`:\n",
    "\n",
    "1. Build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the R2 scores of each of the models and compare to the linear regression earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualizing the regression tree decisions\n",
    "\n",
    "Use the template code below to create charts that show the logic/branching of your four decision tree regressions from above.\n",
    "\n",
    "#### Interpreting a regression tree diagram\n",
    "\n",
    "- First line is the condition used to split that node (go left if true, go right if false)\n",
    "- `samples` is the number of observations in that node before splitting\n",
    "- `mse` is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- `value` is the mean response value in that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPLATE CODE\n",
    "# from six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "# from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "\n",
    "# # initialize the output file object\n",
    "# dot_data = StringIO() \n",
    "\n",
    "# # my fit DecisionTreeRegressor object here is: dtr1\n",
    "# # for feature_names i put the columns of my Xr matrix\n",
    "# export_graphviz(dtr1, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True,\n",
    "#                 feature_names=Xr.columns)  \n",
    "\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Building classification trees\n",
    "\n",
    "With `DecisionTreeClassifier`:\n",
    "\n",
    "1. Again build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the accuracy scores of each of the models and compare to the logistic regression earlier.\n",
    "\n",
    "Note that now you'll be using the classification task where we are predicting `admit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize the classification trees\n",
    "\n",
    "The plotting code will be the same as for regression, you just need to change the model you're using for each plot and the feature names.\n",
    "\n",
    "The output changes somewhat from the regression tree chart. Earlier it would give the MSE of that node, but now there is a line called `value` that tells you the count of each class at that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Using GridSearchCV to find the best decision tree classifier\n",
    "\n",
    "As decision trees that are unrestricted will just end up overfitting the training data, decision tree regression and classification models in sklearn offer a variety of ways to \"pre-prune\" (by restricting how many times the tree can branch and what it can use).\n",
    "\n",
    "Measure           | What it does\n",
    "------------------|-------------\n",
    "max_depth         | How many nodes deep can the decision tree go?\n",
    "max_features      | Is there a cutoff to the number of features to use?\n",
    "max_leaf_nodes    | How many leaves can be generated per tree?\n",
    "min_samples_leaf  | How many samples need to be included at a leaf, at a minimum?  \n",
    "min_samples_split | How many samples need to be included at a node, at a minimum?\n",
    "ccp_alpha         | Associate a cost with the number of terminal nodes\n",
    "\n",
    "It is not always best to search over _all_ of these in a grid search, unless you have a small dataset. Many of them while not redundant are going to have very similar effects on your model's fit.\n",
    "\n",
    "Check out the documentation here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the grid search for the regression and classification decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch over to the college stats dataset\n",
    "\n",
    "We are going to be predicting whether or not a college is public or private. Set up your X, y variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.read_csv('../../../../resource-datasets/college_stats/College.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Set up and run the gridsearch on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Print out the \"feature importances\"\n",
    "\n",
    "The model has an attribute called `.feature_importances_` which will rank the features according to their importance. The ranking is based on an importance measure ranging from 0 to 1, with 1 being the most important. The importance scores of all features add up to 1.\n",
    "\n",
    "The score takes into account how many times the feature was used to make a decision, how many data points were involved in each decision and how much the decision increased the purity of the node. A feature with higher feature importance reduced the criterion (impurity) more than the other features.\n",
    "\n",
    "Below, show the feature importances for each variable predicting private versus not, sorted by most important feature to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
