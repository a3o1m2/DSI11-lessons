{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Support Vector Machines (SVM)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how the SVM builds its decision threshold\n",
    "- Understand the concept of the maximum margin hyperplane\n",
    "- Visualize the linearly separable case in classification\n",
    "- Understand the hinge loss for SVM\n",
    "- Understand how the regularization constant C allows SVMs to fit non-linearly separable problems\n",
    "- See how the kernel trick transforms problems from non-linearly separable to linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-0.1\">Learning Objectives</a></span></li></ul></li><li><span><a href=\"#Introduction-to-SVMs\" data-toc-modified-id=\"Introduction-to-SVMs-1\">Introduction to SVMs</a></span></li><li><span><a href=\"#How-does-the-SVM-classify?\" data-toc-modified-id=\"How-does-the-SVM-classify?-2\">How does the SVM classify?</a></span></li><li><span><a href=\"#Intuition-behind-the-SVM-decision-boundary\" data-toc-modified-id=\"Intuition-behind-the-SVM-decision-boundary-3\">Intuition behind the SVM decision boundary</a></span></li><li><span><a href=\"#Why-maximize-the-margin?\" data-toc-modified-id=\"Why-maximize-the-margin?-4\">Why maximize the margin?</a></span></li><li><span><a href=\"#Hinge-loss\" data-toc-modified-id=\"Hinge-loss-5\">Hinge loss</a></span></li><li><span><a href=\"#Hinge-loss-and-&quot;slack&quot;\" data-toc-modified-id=\"Hinge-loss-and-&quot;slack&quot;-6\">Hinge loss and \"slack\"</a></span></li><li><span><a href=\"#The-&quot;kernel-trick&quot;-for-non-linearly-separable-problems\" data-toc-modified-id=\"The-&quot;kernel-trick&quot;-for-non-linearly-separable-problems-7\">The \"kernel trick\" for non-linearly separable problems</a></span></li><li><span><a href=\"#Kernel-functions\" data-toc-modified-id=\"Kernel-functions-8\">Kernel functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kernel-Example\" data-toc-modified-id=\"Kernel-Example-8.1\">Kernel Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Kernel\" data-toc-modified-id=\"Linear-Kernel-8.1.1\">Linear Kernel</a></span></li><li><span><a href=\"#Polynomial-Kernel\" data-toc-modified-id=\"Polynomial-Kernel-8.1.2\">Polynomial Kernel</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-8.1.3\">Solution (double click)</a></span></li><li><span><a href=\"#RBF-Kernel\" data-toc-modified-id=\"RBF-Kernel-8.1.4\">RBF Kernel</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-8.1.5\">Solution (double click)</a></span></li><li><span><a href=\"#Sigmoid-Kernel\" data-toc-modified-id=\"Sigmoid-Kernel-8.1.6\">Sigmoid Kernel</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-8.1.7\">Solution (double click)</a></span></li></ul></li></ul></li><li><span><a href=\"#Making-predictions\" data-toc-modified-id=\"Making-predictions-9\">Making predictions</a></span></li><li><span><a href=\"#Additional-resources\" data-toc-modified-id=\"Additional-resources-10\">Additional resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SVMs\n",
    "\n",
    "---\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is a different approach to classification.\n",
    "\n",
    "SVM still fits a decision boundary like a logistic regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "This lesson will overview the details of the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does the SVM classify?\n",
    "\n",
    "---\n",
    "\n",
    "It's important to start with the intuition for SVM with the special _**linearly separable**_ classification case.\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./assets/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why maximize the margin?\n",
    "\n",
    "---\n",
    "\n",
    "**SVM solves for a decision boundary that should theoretically minimize the generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the observations with the most \"ambiguous\" labels. They are the observations that are approaching equal probability to be one class or the other (given the predictors).\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines its fit using the most ambiguous points. Its decision boundary is safe in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss\n",
    "\n",
    "In practice, most often we will not have linear separability. Some points will enter into the margin, some might even be on the wrong side. Then the location of the decision boundary is found by optimizing the hinge loss. Observations which are outside the margin and on the correct side do not contribute to the loss. Those instead which are inside the margin or on the wrong side do contribute to the loss and are part of the support vectors.\n",
    "\n",
    "![](assets/hinge_loss_plt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinge loss and \"slack\"\n",
    "\n",
    "When we have a scenario where it is not possible to perfectly separate, we allow some of the points to be on the wrong side by an amount $\\xi_i\\ge 0$ (we do not measure how much points are on the correct side). \n",
    "We will allow only for a certain total budget of being on the wrong side:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N\\xi_i\\le {\\rm constant}\n",
    "$$\n",
    "\n",
    "We control this budget using a regularization constant $C$. Large values of $C$ strongly penalize being on the wrong side, low values of $C$ hardly penalize being on the wrong side. The $\\xi_i$ are called **slack variables**.\n",
    "\n",
    "In this way, we try to balance between how wide the margin should be and how much error we tolerate. Doing so allows the SVM to classify non-linearly separable problems by allowing errors to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For those interested in exploring the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/slack_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/soft_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The \"kernel trick\" for non-linearly separable problems\n",
    "\n",
    "---\n",
    "\n",
    "The \"kernel trick\" allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "The idea behind the kernel trick is that you can arbitrarily transform your observations that _have no linear separability_ by putting them into a different \"dimensional space\" where they DO have linear separability, fit an SVM in that higher dimensional space, and then invert the transformation of the data and the model itself back into the original space.\n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them into this higher dimensional space. \n",
    "\n",
    "[Check out these lecture slides for more detail.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you a general intuition for what is happening.\n",
    "\n",
    "</n>\n",
    "<font color = red>\n",
    "You would have had the ability to separate this in such a way if you had more features in order to make the differentiation, but if you only have two features like here... you can wrap the features you do have inside a kernel function which would add the extra polynomial features in order to enable delineation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./assets/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./assets/nonlinear-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel functions\n",
    "\n",
    "Linear Kernel:\n",
    "\n",
    "$$\n",
    "K(x, x') = \\langle x,x'\\rangle = \\sum_{i=1}^p x_i x'_i\n",
    "$$\n",
    "\n",
    "Here,\n",
    "$$\n",
    "\\langle x,x'\\rangle = \\sum_{i=1}^p x_i x'_i\n",
    "$$\n",
    "stands for the scalar product.\n",
    "\n",
    "\n",
    "dth-degree polynomial:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\left(r+\\gamma\\langle x,x'\\rangle\\right)^d\n",
    "$$\n",
    "\n",
    "radial basis:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\exp\\left(-\\gamma \\|x-x'\\|^2\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "sigmoid:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\tanh\\left(\\gamma\\langle x,x'\\rangle + r\\right)\n",
    "$$\n",
    "\n",
    "Here,\n",
    "$d$, $\\gamma$, $r$ are tuning constants.\n",
    "\n",
    "Sklearn chooses the following names for these parameters in its SVM-classifier:\n",
    "\n",
    "- $d$ $\\rightarrow$ degree (default 3)\n",
    "- $\\gamma$ $\\rightarrow$ gamma (default 1)\n",
    "- $r$ $\\rightarrow$ coef0 (default 0)\n",
    "\n",
    "The parameters only have an effect if they appear in the kernel definition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Example\n",
    "\n",
    "Consider some points in a 2-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 244,
       "width": 409
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[0, 1], \n",
    "              [1, 0], \n",
    "              [2, 0]])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, rbf_kernel, sigmoid_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernel\n",
    "\n",
    "The kernel function produces as an output a matrix containing the results of the kernel function applied to all pairwise combinations of observations. For example at position `[0, 0]`, we have the kernel function calculated between observation 0 and itself, at position `[0, 1]` we have the kernel function calculated between observation 0 and observation 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_kernel(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reconstruct how these values are obtained, let's use the kernel definition iterating over all possible pairs of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 1\n",
      "(0, 1) 0\n",
      "(0, 2) 0\n",
      "(1, 0) 0\n",
      "(1, 1) 1\n",
      "(1, 2) 2\n",
      "(2, 0) 0\n",
      "(2, 1) 2\n",
      "(2, 2) 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), np.dot(X[i], X[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the computation with scikit-learn using other kernel functions. Have a look at the kernel implementations. Try to reconstruct all matrix elements using the formulas from above. Play around with the kernel parameters.\n",
    "\n",
    "> **Hints:**\n",
    "- Calculate $\\|x-x'\\|$ with `np.linalg.norm(x-x')`.\n",
    "- $\\tanh(x)$ is calculated with `np.tanh(x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  1.,  1.],\n",
       "       [ 1.,  4.,  9.],\n",
       "       [ 1.,  9., 25.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef0 = 1\n",
    "gamma = 1\n",
    "degree = 2\n",
    "pk = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n",
    "pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dth-degree polynomial:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\left(r+\\gamma\\langle x,x'\\rangle\\right)^d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 4\n",
      "(0, 1) 1\n",
      "(0, 2) 1\n",
      "(1, 0) 1\n",
      "(1, 1) 4\n",
      "(1, 2) 9\n",
      "(2, 0) 1\n",
      "(2, 1) 9\n",
      "(2, 2) 25\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), (coef0+gamma*np.dot(X[i], X[j]))**degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), (coef0+gamma*np.dot(X[i], X[j]))**degree)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.36787944, 0.082085  ],\n",
       "       [0.36787944, 1.        , 0.60653066],\n",
       "       [0.082085  , 0.60653066, 1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "rbf_kernel(X, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "radial basis:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\exp\\left(-\\gamma \\|x-x'\\|^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 1.0\n",
      "(0, 1) 0.3678794411714422\n",
      "(0, 2) 0.08208499862389876\n",
      "(1, 0) 0.3678794411714422\n",
      "(1, 1) 1.0\n",
      "(1, 2) 0.6065306597126334\n",
      "(2, 0) 0.08208499862389876\n",
      "(2, 1) 0.6065306597126334\n",
      "(2, 2) 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), np.exp(-gamma*np.linalg.norm(X[i] - X[j])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), np.exp(-gamma*np.linalg.norm(X[i]-X[j])**2))\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76159416, 0.        , 0.        ],\n",
       "       [0.        , 0.76159416, 0.96402758],\n",
       "       [0.        , 0.96402758, 0.9993293 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1\n",
    "coef0 = 0\n",
    "sigmoid_kernel(X, gamma=gamma, coef0=coef0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\tanh\\left(\\gamma\\langle x,x'\\rangle + r\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 0.7615941559557649\n",
      "(0, 1) 0.0\n",
      "(0, 2) 0.0\n",
      "(1, 0) 0.0\n",
      "(1, 1) 0.7615941559557649\n",
      "(1, 2) 0.9640275800758169\n",
      "(2, 0) 0.0\n",
      "(2, 1) 0.9640275800758169\n",
      "(2, 2) 0.999329299739067\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), np.tanh(gamma*np.dot(X[i], X[j])+coef0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i, j), np.tanh(gamma*np.dot(X[i], X[j])+coef0))\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "A prediction for any point $x$ is made by calculating the kernel for this point paired up with all the support vectors:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\sum_{i=1}^N \\hat\\alpha_i y_i K(x,x_i) + \\hat{\\beta}_0\n",
    "$$\n",
    "\n",
    "The coefficients $\\hat{\\alpha}_i$ are the so-called dual coefficients. They are only different from zero if $x_i$ is a support vector, so the sum effectively runs only over the support vectors. Binary class labels $y_i$ have to be encoded as plus or minus one. Like in AdaBoost, the predicted label is obtained by just checking the sign of the prediction $\\hat{f}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "- [This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n",
    "- SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "- Iris example on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py)\n",
    "- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)\n",
    "- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to SVM. My fav!\n",
    "- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane\n",
    "- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview plus a [20min video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core \"need-to-knows\"\n",
    "- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM\n",
    "- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the highnotes of SVM\n",
    "- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on SVM from CS 229\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=eHsErlPJWUU) (1hr+) from one of my fav lecturers (Dr Yasser) on SVM. He does a followup on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50min) (from MIT Opencoursewar)\n",
    "- A famous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 9000+ times!) on why SVM is a great text classifier\n",
    "- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "168px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
