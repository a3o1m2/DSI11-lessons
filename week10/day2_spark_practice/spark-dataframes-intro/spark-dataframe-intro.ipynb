{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Spark Dataframes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spark-has-an-SQL-interface-into-dataframes-like-Hive\" data-toc-modified-id=\"Spark-has-an-SQL-interface-into-dataframes-like-Hive-1\">Spark has an SQL interface into dataframes like <em>Hive</em></a></span></li><li><span><a href=\"#Spark-data-types\" data-toc-modified-id=\"Spark-data-types-2\">Spark data types</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD's\" data-toc-modified-id=\"RDD's-2.1\">RDD's</a></span></li><li><span><a href=\"#DataFrames\" data-toc-modified-id=\"DataFrames-2.2\">DataFrames</a></span></li><li><span><a href=\"#Common-DataFrame-operations-and-characteristics\" data-toc-modified-id=\"Common-DataFrame-operations-and-characteristics-2.3\">Common DataFrame operations and characteristics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-variable-/-column-space-of-a-DataFrame\" data-toc-modified-id=\"Inspect-variable-/-column-space-of-a-DataFrame-2.3.1\">Inspect variable / column space of a DataFrame</a></span></li><li><span><a href=\"#DTypes\" data-toc-modified-id=\"DTypes-2.3.2\">DTypes</a></span></li><li><span><a href=\"#Explain-DataFrame\" data-toc-modified-id=\"Explain-DataFrame-2.3.3\">Explain DataFrame</a></span></li><li><span><a href=\"#Describe\" data-toc-modified-id=\"Describe-2.3.4\">Describe</a></span></li><li><span><a href=\"#printSchema\" data-toc-modified-id=\"printSchema-2.3.5\">printSchema</a></span></li><li><span><a href=\"#Count\" data-toc-modified-id=\"Count-2.3.6\">Count</a></span></li></ul></li><li><span><a href=\"#Some-basic-stats-in-spark\" data-toc-modified-id=\"Some-basic-stats-in-spark-2.4\">Some basic stats in spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Covariance\" data-toc-modified-id=\"Covariance-2.4.1\">Covariance</a></span></li><li><span><a href=\"#Pearson-correlation\" data-toc-modified-id=\"Pearson-correlation-2.4.2\">Pearson correlation</a></span></li></ul></li><li><span><a href=\"#Limiting-results\" data-toc-modified-id=\"Limiting-results-2.5\">Limiting results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Head\" data-toc-modified-id=\"Head-2.5.1\">Head</a></span></li><li><span><a href=\"#Show-limited-results\" data-toc-modified-id=\"Show-limited-results-2.5.2\">Show limited results</a></span></li></ul></li><li><span><a href=\"#-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?\" data-toc-modified-id=\"-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?-2.6\"><i class=\"fa fa-question-circle\"></i> Why should you need to be careful when displaying data in Spark?</a></span></li><li><span><a href=\"#More-DataFrame-and-Series-operations\" data-toc-modified-id=\"More-DataFrame-and-Series-operations-2.7\">More DataFrame and Series operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform-dataframe-to-RDD\" data-toc-modified-id=\"Transform-dataframe-to-RDD-2.7.1\">Transform dataframe to RDD</a></span></li><li><span><a href=\"#Convert-from-list-of-Row-objects-to-list-of-dictionaries\" data-toc-modified-id=\"Convert-from-list-of-Row-objects-to-list-of-dictionaries-2.7.2\">Convert from list of Row objects to list of dictionaries</a></span></li><li><span><a href=\"#Selecting-DataFrame-Series\" data-toc-modified-id=\"Selecting-DataFrame-Series-2.7.3\">Selecting DataFrame Series</a></span></li><li><span><a href=\"#Select-all-features-/-variables-/-columns\" data-toc-modified-id=\"Select-all-features-/-variables-/-columns-2.7.4\">Select all features / variables / columns</a></span></li><li><span><a href=\"#Select-specific-features-/-variables-/-columns\" data-toc-modified-id=\"Select-specific-features-/-variables-/-columns-2.7.5\">Select specific features / variables / columns</a></span></li><li><span><a href=\"#Series-Operations\" data-toc-modified-id=\"Series-Operations-2.7.6\">Series Operations</a></span></li><li><span><a href=\"#As-an-&quot;alias&quot;\" data-toc-modified-id=\"As-an-&quot;alias&quot;-2.7.7\">As an \"alias\"</a></span></li><li><span><a href=\"#Creating-new-features-/-variables-/-columns\" data-toc-modified-id=\"Creating-new-features-/-variables-/-columns-2.7.8\">Creating new features / variables / columns</a></span></li></ul></li><li><span><a href=\"#-Have-we-changed-the-original-DataFrame?\" data-toc-modified-id=\"-Have-we-changed-the-original-DataFrame?-2.8\"><i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-Data\" data-toc-modified-id=\"Filtering-Data-2.8.1\">Filtering Data</a></span></li><li><span><a href=\"#Multiple-conditions\" data-toc-modified-id=\"Multiple-conditions-2.8.2\">Multiple conditions</a></span></li><li><span><a href=\"#Filter-as-an-expression\" data-toc-modified-id=\"Filter-as-an-expression-2.8.3\">Filter as an expression</a></span></li><li><span><a href=\"#Sorting\" data-toc-modified-id=\"Sorting-2.8.4\">Sorting</a></span></li><li><span><a href=\"#Creating-a-histogram-from-a-column\" data-toc-modified-id=\"Creating-a-histogram-from-a-column-2.8.5\">Creating a histogram from a column</a></span></li></ul></li><li><span><a href=\"#AS-SQL!?\" data-toc-modified-id=\"AS-SQL!?-2.9\">AS SQL!?</a></span></li><li><span><a href=\"#Temporary-Views-select-DataFrames\" data-toc-modified-id=\"Temporary-Views-select-DataFrames-2.10\">Temporary Views select DataFrames</a></span></li></ul></li><li><span><a href=\"#Check-out-another-dataset-using-Spark-DataFrames\" data-toc-modified-id=\"Check-out-another-dataset-using-Spark-DataFrames-3\">Check out another dataset using Spark DataFrames</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset\" data-toc-modified-id=\"1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset-3.0.1\">1. Load up the \"Pokemon\" basic Pokedex dataset</a></span></li><li><span><a href=\"#2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.\" data-toc-modified-id=\"2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.-3.0.2\">2. Check out the dataset with infer schema parameter but without header.</a></span></li><li><span><a href=\"#3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;\" data-toc-modified-id=\"3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;-3.0.3\">3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"</a></span></li><li><span><a href=\"#4.a-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.a-Which-is-the-strongest-Pokemon-by-Type?-3.0.4\">4.a Which is the strongest Pokemon by <code>Type</code>?</a></span></li><li><span><a href=\"#4.b-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.b-Which-is-the-strongest-Pokemon-by-Type?-3.0.5\">4.b Which is the strongest Pokemon by Type?</a></span></li><li><span><a href=\"#5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-3.0.6\">5.a Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-3.0.7\">5.b Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.\" data-toc-modified-id=\"6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.-3.0.8\">6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP.</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark has an SQL interface into dataframes like _Hive_\n",
    "\n",
    "Spark isn't exactly **Hive**, but it uses components from Hive.  You can use temporary SQL views with Spark dataframes.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then you can slice and dice your dataframe with SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark data types\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD's\n",
    "\n",
    "It's best to think of RDDs as primitive objects that are distributed.  RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "The big difference between RDD's and DataFrames is that DataFrames introduce the idea of a \"schema\" much like Pandas.  \n",
    "\n",
    "The big plus is that Spark DataFrames serialize data at a lower level to native Java/Scala, so when it's passed between nodes, it's much more performant, requiring fewer processes to handle computations.  Mainly data can be processed faster when it's optimized to a common format (the schema) that Spark doesn't have to convert to in order to perform tasks on it.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based datastructure, the **DataFrame API** provides a convenient set of selectors for transforming data, much like Pandas.  Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL - **SparkSQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext(\"local\")\n",
    "spark = ps.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path=\"data/sentiment_words_simple.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the dataframe, we have to use the command `.show()`. It will limit to 20 rows by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common DataFrame operations and characteristics\n",
    "---\n",
    "\n",
    "Let's have a look at some familiar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect variable / column space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTypes\n",
    "\n",
    "Inspect schema programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe\n",
    "Describe will look similar to the synonymous Pandas **describe** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|summary|   pos|                word|          pos_score|          neg_score|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|  count|155287|              155287|             155287|             155287|\n",
      "|   mean|  null| 5.687506389495568E9|0.03865380738372139|0.05048873043068892|\n",
      "| stddev|  null|7.537744261701462E10|0.11118246263109259| 0.1392276319041252|\n",
      "|    min|   adj|               'hood|                0.0|                0.0|\n",
      "|    max|  verb|              zyrian|                1.0|                1.0|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>count</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>5.687506389495568E9</td>\n",
       "      <td>0.03865380738372139</td>\n",
       "      <td>0.05048873043068892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>7.537744261701462E10</td>\n",
       "      <td>0.11118246263109259</td>\n",
       "      <td>0.1392276319041252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>min</td>\n",
       "      <td>adj</td>\n",
       "      <td>'hood</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>max</td>\n",
       "      <td>verb</td>\n",
       "      <td>zyrian</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary     pos                  word            pos_score  \\\n",
       "0   count  155287                155287               155287   \n",
       "1    mean    None   5.687506389495568E9  0.03865380738372139   \n",
       "2  stddev    None  7.537744261701462E10  0.11118246263109259   \n",
       "3     min     adj                 'hood                  0.0   \n",
       "4     max    verb                zyrian                  1.0   \n",
       "\n",
       "             neg_score  \n",
       "0               155287  \n",
       "1  0.05048873043068892  \n",
       "2   0.1392276319041252  \n",
       "3                  0.0  \n",
       "4                  1.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>155287.000000</td>\n",
       "      <td>155287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.038654</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.139228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pos_score      neg_score\n",
       "count  155287.000000  155287.000000\n",
       "mean        0.038654       0.050489\n",
       "std         0.111182       0.139228\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.000000       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Pandas version here!!!\n",
    "df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printSchema\n",
    "The schema is a very import characteristic of a Spark DataFrame.  It tells us what's possible in terms of transformation.  Also, it's the reason DataFrames are so fast since they are typed to a set number of types that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The \"schema\" that we've been so excited to see is finally here to explore.  Feel free to take a screenshot and show your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PokedexNumber: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- SpecialAttack: integer (nullable = true)\n",
      " |-- SpecialDefense: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count\n",
    "Count with caveat:  This will return the count of all rows, including _non-NaN_ values.  Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats in spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data is loaded into an instance's memory -- use for small datasets\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show limited results\n",
    "\n",
    "With Pandas we're used to the `df.head()` as a first step in exploring a dataset.  With Spark this isn't exactly the same.  You need to use the `df.show()` operation in order to explore data as a first step.  Where Pandas formats its DataFrame output for display in nice HTML tables with sensible defaults for output, you have to be a bit more specific about what you're looking at with `show()` when using Spark.\n",
    "\n",
    "\n",
    "> The parameter `truncate` is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(19).show()\n",
    "# `show()` can also be chained to certain outputs like `limit`.\n",
    "# `show` by itself is a compound operation for displaying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why should you need to be careful when displaying data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so nice to use for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More DataFrame and Series operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform dataframe to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from list of Row objects to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row.asDict() for row in df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting DataFrame Series\n",
    "\n",
    "Selecting variables with Spark **DataFrames API** works very similarly to Pandas DataFrames.  When selecting variables in Pandas use a `list` object, passed to a DataFrame object via `[]` brackets like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in spark is using the `.select()` method which takes flat parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"word\", \"pos_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"** and **\"col2\"** with\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df[\"pos_score\"] + df[\"neg_score\"]).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do math operations in series as well\n",
    "df.select(df[\"pos_score\"] + 10).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"alias\"\n",
    "As selections are keyed by conditions, they can become hard to read.  We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select((df[\"pos_score\"] + df[\"neg_score\"]).alias(\"total_score\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+----------+\n",
      "|pos|       word|pos_score|neg_score|new_column|\n",
      "+---+-----------+---------+---------+----------+\n",
      "|adj|.22-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|          0|      0.0|      0.5|       0.5|\n",
      "|adj|          1|      0.0|     0.25|      0.25|\n",
      "|adj|         10|      0.0|      0.0|       0.0|\n",
      "+---+-----------+---------+---------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['pos_score'] + df['neg_score'])\n",
    "df.withColumn(\"new_column\", df['pos_score'] + df['neg_score']).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas we use \"masks\" through dataframe object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"pos_score\"] > .5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple conditions\n",
    "SAME AS PANDAS!  Thankfully, we don't have to leave our comfort zone with too many oddities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df[\"pos_score\"] > .5) & (df[\"neg_score\"] > 0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df[\"pos_score\"] > .5) \\\n",
    "    .filter(df[\"neg_score\"] > 0).show(5)  # Filters can be chained per line using the \\ newline escape sequence character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an expression\n",
    "Pandas has a similar function called \"where\".  However, with Spark `filter`, we can filter by shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5 AND neg_score > 0\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condition = \"\"\"\n",
    "pos_score > .5 AND \n",
    "neg_score > 0 \n",
    "\"\"\"\n",
    "\n",
    "df.filter(condition).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.neg_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.pos_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.desc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.asc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a histogram from a column\n",
    "\n",
    "We select a single column, transform it into an RDD, flatten it, and can then apply the RDD histogram function which returns arrays with bin-boundaries and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.1,\n",
       "  0.2,\n",
       "  0.30000000000000004,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.6000000000000001,\n",
       "  0.7000000000000001,\n",
       "  0.8,\n",
       "  0.9,\n",
       "  1.0],\n",
       " [131707, 8151, 5241, 3510, 418, 2831, 2029, 1109, 280, 11])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.select('neg_score').rdd.flatMap(lambda x: x)\n",
    "histogram = a.histogram(10)\n",
    "histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 259,
       "width": 387
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(histogram[0][:-1], histogram[1], width=np.diff(histogram[0]), ec=\"k\", align=\"edge\")\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AS SQL!?\n",
    "\n",
    "\n",
    "Working with DataFrames and SQL is as easy as creating a **temporary view**. In this way you define the table name to refer to in SQL-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|PokedexNumber|                Name|       Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|            1|           Bulbasaur|GrassPoison|  318| 45|    49|     49|           65|            65|   45|\n",
      "|            2|             Ivysaur|GrassPoison|  405| 60|    62|     63|           80|            80|   60|\n",
      "|            3|            Venusaur|GrassPoison|  525| 80|    82|     83|          100|           100|   80|\n",
      "|            3|VenusaurMega Venu...|GrassPoison|  625| 80|   100|    123|          122|           120|   80|\n",
      "|            4|          Charmander|       Fire|  309| 39|    52|     43|           60|            50|   65|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = spark.sql(\"SELECT * FROM sentiment LIMIT 100\")\n",
    "sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.toPandas()['neg_score'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views select DataFrames\n",
    "So the same transformations can be applied to DataFrames as we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+---------+-------------------+\n",
      "|summary| pos|              word|pos_score|          neg_score|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "|  count| 100|               100|      100|                100|\n",
      "|   mean|null|  96.9090909090909|      0.0|            0.02625|\n",
      "| stddev|null|154.37463012769066|      0.0|0.09282158132458183|\n",
      "|    min| adj|       .22-caliber|      0.0|                0.0|\n",
      "|    max| adj|              29th|      0.0|              0.625|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out another dataset using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load up the \"Pokemon\" basic Pokedex dataset\n",
    "First try without inferring the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\n",
    "    path=\"./data/pokedex_basic.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "|summary|PokedexNumber| Name| Type|Total|   HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|\n",
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "|  count|          800|  800|  800|  800|  800|   800|    800|          800|           800|  800|\n",
      "|   mean|        36...| null| null|43...|69...| 79...|  73...|        72.82|         71...|68...|\n",
      "|  st...|        20...| null| null|11...|25...| 32...|  31...|        32...|         27...|29...|\n",
      "|    min|            1|Ab...|  Bug|  180|    1|     5|      5|           10|            20|    5|\n",
      "|    max|          721|Zy...|Wa...|  780|  255|   190|    230|          194|           230|  180|\n",
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check out the dataset with infer schema parameter but without header.\n",
    "How does it work with / without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PokedexNumber: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- SpecialAttack: integer (nullable = true)\n",
      " |-- SpecialDefense: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(PokedexNumber,IntegerType,true),StructField(Name,StringType,true),StructField(Type,StringType,true),StructField(Total,IntegerType,true),StructField(HP,IntegerType,true),StructField(Attack,IntegerType,true),StructField(Defense,IntegerType,true),StructField(SpecialAttack,IntegerType,true),StructField(SpecialDefense,IntegerType,true),StructField(Speed,IntegerType,true)))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|PokedexNumber|                Name|       Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|            1|           Bulbasaur|GrassPoison|  318| 45|    49|     49|           65|            65|   45|\n",
      "|            2|             Ivysaur|GrassPoison|  405| 60|    62|     63|           80|            80|   60|\n",
      "|            3|            Venusaur|GrassPoison|  525| 80|    82|     83|          100|           100|   80|\n",
      "|            3|VenusaurMega Venu...|GrassPoison|  625| 80|   100|    123|          122|           120|   80|\n",
      "|            4|          Charmander|       Fire|  309| 39|    52|     43|           60|            50|   65|\n",
      "|            5|          Charmeleon|       Fire|  405| 58|    64|     58|           80|            65|   80|\n",
      "|            6|           Charizard| FireFlying|  534| 78|    84|     78|          109|            85|  100|\n",
      "|            6|CharizardMega Cha...| FireDragon|  634| 78|   130|    111|          130|            85|  100|\n",
      "|            6|CharizardMega Cha...| FireFlying|  634| 78|   104|     78|          159|           115|  100|\n",
      "|            7|            Squirtle|      Water|  314| 44|    48|     65|           50|            64|   43|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.createOrReplaceTempView(\"pokemon\")\n",
    "sqlContext.sql(\"SELECT * FROM pokemon LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a Which is the strongest Pokemon by `Type`?\n",
    "Using Spark DataFrame operations.  Research Spark's \"groupBy\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|           Type|max(attack)|\n",
      "+---------------+-----------+\n",
      "|PsychicFighting|        190|\n",
      "|    BugFighting|        185|\n",
      "|        Psychic|        180|\n",
      "|   DragonFlying|        180|\n",
      "|     GroundFire|        180|\n",
      "+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df = df.select('Type', 'attack').groupBy(\n",
    "    'Type').max().sort('max(attack)', ascending=False)\n",
    "count_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_df.withColumn('attack', count_df['max(attack)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------+\n",
      "|                Name|           Type|attack|\n",
      "+--------------------+---------------+------+\n",
      "| MewtwoMega Mewtwo X|PsychicFighting|   190|\n",
      "|HeracrossMega Her...|    BugFighting|   185|\n",
      "|GroudonPrimal Gro...|     GroundFire|   180|\n",
      "|RayquazaMega Rayq...|   DragonFlying|   180|\n",
      "|  DeoxysAttack Forme|        Psychic|   180|\n",
      "|  KyuremBlack Kyurem|      DragonIce|   170|\n",
      "|GarchompMega Garc...|   DragonGround|   170|\n",
      "| BanetteMega Banette|          Ghost|   165|\n",
      "|           Rampardos|           Rock|   165|\n",
      "|TyranitarMega Tyr...|       RockDark|   164|\n",
      "|             Slaking|         Normal|   160|\n",
      "|           Regigigas|         Normal|   160|\n",
      "| DiancieMega Diancie|      RockFairy|   160|\n",
      "|  HoopaHoopa Unbound|    PsychicDark|   160|\n",
      "|BlazikenMega Blaz...|   FireFighting|   160|\n",
      "|   PinsirMega Pinsir|      BugFlying|   155|\n",
      "|GyaradosMega Gyar...|      WaterDark|   155|\n",
      "|SwampertMega Swam...|    WaterGround|   150|\n",
      "|   ScizorMega Scizor|       BugSteel|   150|\n",
      "|BeedrillMega Beed...|      BugPoison|   150|\n",
      "+--------------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df.join(df, on=['Type', 'attack']).select(\n",
    "    'Name', 'Type', 'attack').sort('attack', ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Which is the strongest Pokemon by Type?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+\n",
      "|                Name|           Type|TotalMax|\n",
      "+--------------------+---------------+--------+\n",
      "| MewtwoMega Mewtwo X|PsychicFighting|     190|\n",
      "|HeracrossMega Her...|    BugFighting|     185|\n",
      "|GroudonPrimal Gro...|     GroundFire|     180|\n",
      "|  DeoxysAttack Forme|        Psychic|     180|\n",
      "|RayquazaMega Rayq...|   DragonFlying|     180|\n",
      "|GarchompMega Garc...|   DragonGround|     170|\n",
      "|  KyuremBlack Kyurem|      DragonIce|     170|\n",
      "|           Rampardos|           Rock|     165|\n",
      "| BanetteMega Banette|          Ghost|     165|\n",
      "|TyranitarMega Tyr...|       RockDark|     164|\n",
      "+--------------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.Name, tbl.Type, tbl.TotalMax\n",
    "FROM (\n",
    "    SELECT Type, MAX(attack) AS TotalMax\n",
    "    FROM pokemon\n",
    "    GROUP BY Type\n",
    ") AS tbl \n",
    "LEFT OUTER JOIN pokemon p \n",
    "ON tbl.Type = p.Type AND tbl.TotalMax = p.attack\n",
    "ORDER BY TotalMax DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "strongest_pokemon = sqlContext.sql(sql)\n",
    "strongest_pokemon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Which Pokemon has the best combined Attack and Defence?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|                Name|       Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Poketotal|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          306|   AggronMega Aggron|      Steel|  630| 70|   140|    230|           60|            80|   50|      370|\n",
      "|          208| SteelixMega Steelix|SteelGround|  610| 75|   125|    230|           55|            95|   30|      355|\n",
      "|          383|GroudonPrimal Gro...| GroundFire|  770|100|   180|    160|          150|            90|   90|      340|\n",
      "|          248|TyranitarMega Tyr...|   RockDark|  700|100|   164|    150|           95|           120|   71|      314|\n",
      "|          713|             Avalugg|        Ice|  514| 95|   117|    184|           44|            46|   28|      301|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.withColumn(\"Poketotal\", df[\"Attack\"] + df[\"Defense\"]).sort(\"Poketotal\", ascending=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Which Pokemon has the best combined Attack and Defence?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|                Name|           Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Poketotal|\n",
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          306|   AggronMega Aggron|          Steel|  630| 70|   140|    230|           60|            80|   50|      370|\n",
      "|          208| SteelixMega Steelix|    SteelGround|  610| 75|   125|    230|           55|            95|   30|      355|\n",
      "|          383|GroudonPrimal Gro...|     GroundFire|  770|100|   180|    160|          150|            90|   90|      340|\n",
      "|          248|TyranitarMega Tyr...|       RockDark|  700|100|   164|    150|           95|           120|   71|      314|\n",
      "|          713|             Avalugg|            Ice|  514| 95|   117|    184|           44|            46|   28|      301|\n",
      "|          377|            Regirock|           Rock|  580| 80|   100|    200|           50|           100|   50|      300|\n",
      "|          214|HeracrossMega Her...|    BugFighting|  600| 80|   185|    115|           40|           105|   75|      300|\n",
      "|          376|MetagrossMega Met...|   SteelPsychic|  700| 80|   145|    150|          105|           110|  110|      295|\n",
      "|          212|   ScizorMega Scizor|       BugSteel|  600| 70|   150|    140|           65|           100|   75|      290|\n",
      "|          150| MewtwoMega Mewtwo X|PsychicFighting|  780|106|   190|    100|          154|           100|  130|      290|\n",
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, p.Attack + p.Defense AS Poketotal\n",
    "FROM pokemon p\n",
    "ORDER BY Poketotal DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|           Name|           Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Pokevalue|\n",
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          289|        Slaking|         Normal|  670|150|   160|    100|           95|            65|  100|   7800.0|\n",
      "|          383|GroudonPrima...|     GroundFire|  770|100|   180|    160|          150|            90|   90|   6800.0|\n",
      "|          646|KyuremBlack ...|      DragonIce|  700|125|   170|    100|          120|            90|   95|   6750.0|\n",
      "|          487|GiratinaAlte...|    GhostDragon|  680|150|   100|    120|          100|           120|   90|   6600.0|\n",
      "|          487|GiratinaOrig...|    GhostDragon|  680|150|   120|    100|          120|           100|   90|   6600.0|\n",
      "|          248|TyranitarMeg...|       RockDark|  700|100|   164|    150|           95|           120|   71|   6280.0|\n",
      "|          464|      Rhyperior|     GroundRock|  535|115|   140|    130|           55|            55|   40|   6210.0|\n",
      "|          445|GarchompMega...|   DragonGround|  700|108|   170|    115|          120|            95|   92|   6156.0|\n",
      "|          150|MewtwoMega M...|PsychicFighting|  780|106|   190|    100|          154|           100|  130|   6148.0|\n",
      "|          486|      Regigigas|         Normal|  670|110|   160|    110|           80|           110|  100|   5940.0|\n",
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, (p.Attack + p.Defense) * (p.HP * .2) AS Pokevalue\n",
    "FROM pokemon p\n",
    "ORDER BY Pokevalue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(truncate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "579px",
    "left": "561.111px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
