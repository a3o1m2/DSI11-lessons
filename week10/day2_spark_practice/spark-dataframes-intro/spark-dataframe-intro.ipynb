{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Spark Dataframes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spark-has-an-SQL-interface-into-dataframes-like-Hive\" data-toc-modified-id=\"Spark-has-an-SQL-interface-into-dataframes-like-Hive-1\">Spark has an SQL interface into dataframes like <em>Hive</em></a></span></li><li><span><a href=\"#Spark-data-types\" data-toc-modified-id=\"Spark-data-types-2\">Spark data types</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD's\" data-toc-modified-id=\"RDD's-2.1\">RDD's</a></span></li><li><span><a href=\"#DataFrames\" data-toc-modified-id=\"DataFrames-2.2\">DataFrames</a></span></li><li><span><a href=\"#Common-DataFrame-operations-and-characteristics\" data-toc-modified-id=\"Common-DataFrame-operations-and-characteristics-2.3\">Common DataFrame operations and characteristics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-variable-/-column-space-of-a-DataFrame\" data-toc-modified-id=\"Inspect-variable-/-column-space-of-a-DataFrame-2.3.1\">Inspect variable / column space of a DataFrame</a></span></li><li><span><a href=\"#DTypes\" data-toc-modified-id=\"DTypes-2.3.2\">DTypes</a></span></li><li><span><a href=\"#Explain-DataFrame\" data-toc-modified-id=\"Explain-DataFrame-2.3.3\">Explain DataFrame</a></span></li><li><span><a href=\"#Describe\" data-toc-modified-id=\"Describe-2.3.4\">Describe</a></span></li><li><span><a href=\"#printSchema\" data-toc-modified-id=\"printSchema-2.3.5\">printSchema</a></span></li><li><span><a href=\"#Count\" data-toc-modified-id=\"Count-2.3.6\">Count</a></span></li></ul></li><li><span><a href=\"#Some-basic-stats-in-spark\" data-toc-modified-id=\"Some-basic-stats-in-spark-2.4\">Some basic stats in spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Covariance\" data-toc-modified-id=\"Covariance-2.4.1\">Covariance</a></span></li><li><span><a href=\"#Pearson-correlation\" data-toc-modified-id=\"Pearson-correlation-2.4.2\">Pearson correlation</a></span></li></ul></li><li><span><a href=\"#Limiting-results\" data-toc-modified-id=\"Limiting-results-2.5\">Limiting results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Head\" data-toc-modified-id=\"Head-2.5.1\">Head</a></span></li><li><span><a href=\"#Show-limited-results\" data-toc-modified-id=\"Show-limited-results-2.5.2\">Show limited results</a></span></li></ul></li><li><span><a href=\"#-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?\" data-toc-modified-id=\"-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?-2.6\"><i class=\"fa fa-question-circle\"></i> Why should you need to be careful when displaying data in Spark?</a></span></li><li><span><a href=\"#More-DataFrame-and-Series-operations\" data-toc-modified-id=\"More-DataFrame-and-Series-operations-2.7\">More DataFrame and Series operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform-dataframe-to-RDD\" data-toc-modified-id=\"Transform-dataframe-to-RDD-2.7.1\">Transform dataframe to RDD</a></span></li><li><span><a href=\"#Convert-from-list-of-Row-objects-to-list-of-dictionaries\" data-toc-modified-id=\"Convert-from-list-of-Row-objects-to-list-of-dictionaries-2.7.2\">Convert from list of Row objects to list of dictionaries</a></span></li><li><span><a href=\"#Selecting-DataFrame-Series\" data-toc-modified-id=\"Selecting-DataFrame-Series-2.7.3\">Selecting DataFrame Series</a></span></li><li><span><a href=\"#Select-all-features-/-variables-/-columns\" data-toc-modified-id=\"Select-all-features-/-variables-/-columns-2.7.4\">Select all features / variables / columns</a></span></li><li><span><a href=\"#Select-specific-features-/-variables-/-columns\" data-toc-modified-id=\"Select-specific-features-/-variables-/-columns-2.7.5\">Select specific features / variables / columns</a></span></li><li><span><a href=\"#Series-Operations\" data-toc-modified-id=\"Series-Operations-2.7.6\">Series Operations</a></span></li><li><span><a href=\"#As-an-&quot;alias&quot;\" data-toc-modified-id=\"As-an-&quot;alias&quot;-2.7.7\">As an \"alias\"</a></span></li><li><span><a href=\"#Creating-new-features-/-variables-/-columns\" data-toc-modified-id=\"Creating-new-features-/-variables-/-columns-2.7.8\">Creating new features / variables / columns</a></span></li></ul></li><li><span><a href=\"#-Have-we-changed-the-original-DataFrame?\" data-toc-modified-id=\"-Have-we-changed-the-original-DataFrame?-2.8\"><i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-Data\" data-toc-modified-id=\"Filtering-Data-2.8.1\">Filtering Data</a></span></li><li><span><a href=\"#Multiple-conditions\" data-toc-modified-id=\"Multiple-conditions-2.8.2\">Multiple conditions</a></span></li><li><span><a href=\"#Filter-as-an-expression\" data-toc-modified-id=\"Filter-as-an-expression-2.8.3\">Filter as an expression</a></span></li><li><span><a href=\"#Sorting\" data-toc-modified-id=\"Sorting-2.8.4\">Sorting</a></span></li><li><span><a href=\"#Creating-a-histogram-from-a-column\" data-toc-modified-id=\"Creating-a-histogram-from-a-column-2.8.5\">Creating a histogram from a column</a></span></li></ul></li><li><span><a href=\"#AS-SQL!?\" data-toc-modified-id=\"AS-SQL!?-2.9\">AS SQL!?</a></span></li><li><span><a href=\"#Temporary-Views-select-DataFrames\" data-toc-modified-id=\"Temporary-Views-select-DataFrames-2.10\">Temporary Views select DataFrames</a></span></li></ul></li><li><span><a href=\"#Check-out-another-dataset-using-Spark-DataFrames\" data-toc-modified-id=\"Check-out-another-dataset-using-Spark-DataFrames-3\">Check out another dataset using Spark DataFrames</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset\" data-toc-modified-id=\"1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset-3.0.1\">1. Load up the \"Pokemon\" basic Pokedex dataset</a></span></li><li><span><a href=\"#2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.\" data-toc-modified-id=\"2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.-3.0.2\">2. Check out the dataset with infer schema parameter but without header.</a></span></li><li><span><a href=\"#3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;\" data-toc-modified-id=\"3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;-3.0.3\">3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"</a></span></li><li><span><a href=\"#4.a-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.a-Which-is-the-strongest-Pokemon-by-Type?-3.0.4\">4.a Which is the strongest Pokemon by <code>Type</code>?</a></span></li><li><span><a href=\"#4.b-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.b-Which-is-the-strongest-Pokemon-by-Type?-3.0.5\">4.b Which is the strongest Pokemon by Type?</a></span></li><li><span><a href=\"#5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-3.0.6\">5.a Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-3.0.7\">5.b Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.\" data-toc-modified-id=\"6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.-3.0.8\">6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP.</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark has an SQL interface into dataframes like _Hive_\n",
    "\n",
    "Spark isn't exactly **Hive**, but it uses components from Hive.  You can use temporary SQL views with Spark dataframes.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then you can slice and dice your dataframe with SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark data types\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD's\n",
    "\n",
    "It's best to think of RDDs as primitive objects that are distributed.  RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "The big difference between RDD's and DataFrames is that DataFrames introduce the idea of a \"schema\" much like Pandas.  \n",
    "\n",
    "The big plus is that Spark DataFrames serialize data at a lower level to native Java/Scala, so when it's passed between nodes, it's much more performant, requiring fewer processes to handle computations.  Mainly data can be processed faster when it's optimized to a common format (the schema) that Spark doesn't have to convert to in order to perform tasks on it.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based datastructure, the **DataFrame API** provides a convenient set of selectors for transforming data, much like Pandas.  Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL - **SparkSQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "sc = ps.SparkContext(\"local\")\n",
    "spark = ps.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path=\"data/sentiment_words_simple.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pos: string, word: string, pos_score: double, neg_score: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the dataframe, we have to use the command `.show()`. It will limit to 20 rows by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common DataFrame operations and characteristics\n",
    "---\n",
    "\n",
    "Let's have a look at some familiar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect variable / column space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'word', 'pos_score', 'neg_score']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTypes\n",
    "\n",
    "Inspect schema programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pos', 'string'),\n",
       " ('word', 'string'),\n",
       " ('pos_score', 'double'),\n",
       " ('neg_score', 'double')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [pos#10,word#11,pos_score#12,neg_score#13] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/crahmede/GA_files/DSI-LDN-2/DSI11-lessons/week10/day2_spark_practic..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pos:string,word:string,pos_score:double,neg_score:double>\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe\n",
    "Describe will look similar to the synonymous Pandas **describe** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|summary|   pos|                word|          pos_score|          neg_score|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|  count|155287|              155287|             155287|             155287|\n",
      "|   mean|  null| 5.687506389495568E9|0.03865380738372139|0.05048873043068892|\n",
      "| stddev|  null|7.537744261701462E10|0.11118246263109259| 0.1392276319041252|\n",
      "|    min|   adj|               'hood|                0.0|                0.0|\n",
      "|    max|  verb|              zyrian|                1.0|                1.0|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "      <td>155287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>5.687506389495568E9</td>\n",
       "      <td>0.03865380738372139</td>\n",
       "      <td>0.05048873043068892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>7.537744261701462E10</td>\n",
       "      <td>0.11118246263109259</td>\n",
       "      <td>0.1392276319041252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>adj</td>\n",
       "      <td>'hood</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>verb</td>\n",
       "      <td>zyrian</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary     pos                  word            pos_score  \\\n",
       "0   count  155287                155287               155287   \n",
       "1    mean    None   5.687506389495568E9  0.03865380738372139   \n",
       "2  stddev    None  7.537744261701462E10  0.11118246263109259   \n",
       "3     min     adj                 'hood                  0.0   \n",
       "4     max    verb                zyrian                  1.0   \n",
       "\n",
       "             neg_score  \n",
       "0               155287  \n",
       "1  0.05048873043068892  \n",
       "2   0.1392276319041252  \n",
       "3                  0.0  \n",
       "4                  1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>155287.000000</td>\n",
       "      <td>155287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038654</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.139228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pos_score      neg_score\n",
       "count  155287.000000  155287.000000\n",
       "mean        0.038654       0.050489\n",
       "std         0.111182       0.139228\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.000000       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Pandas version here!!!\n",
    "df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printSchema\n",
    "The schema is a very import characteristic of a Spark DataFrame.  It tells us what's possible in terms of transformation.  Also, it's the reason DataFrames are so fast since they are typed to a set number of types that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The \"schema\" that we've been so excited to see is finally here to explore.  Feel free to take a screenshot and show your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count\n",
    "Count with caveat:  This will return the count of all rows, including _non-NaN_ values.  Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155287"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats in spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019032076361625806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12294884293406051"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pos='adj', word='.22-caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22-calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.38-caliber', pos_score=0.0, neg_score=0.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the data is loaded into an instance's memory -- use for small datasets\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show limited results\n",
    "\n",
    "With Pandas we're used to the `df.head()` as a first step in exploring a dataset.  With Spark this isn't exactly the same.  You need to use the `df.show()` operation in order to explore data as a first step.  Where Pandas formats its DataFrame output for display in nice HTML tables with sensible defaults for output, you have to be a bit more specific about what you're looking at with `show()` when using Spark.\n",
    "\n",
    "\n",
    "> The parameter `truncate` is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+---------+\n",
      "|pos|    word|pos_score|neg_score|\n",
      "+---+--------+---------+---------+\n",
      "|adj|.22-c...|      0.0|      0.0|\n",
      "|adj|.22-c...|      0.0|      0.0|\n",
      "|adj|.22_c...|      0.0|      0.0|\n",
      "|adj|.22_c...|      0.0|      0.0|\n",
      "|adj|.38-c...|      0.0|      0.0|\n",
      "+---+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|\n",
      "|adj|.45_caliber|      0.0|      0.0|\n",
      "|adj|.45_calibre|      0.0|      0.0|\n",
      "|adj|          0|      0.0|      0.5|\n",
      "|adj|          1|      0.0|     0.25|\n",
      "|adj|         10|      0.0|      0.0|\n",
      "|adj|10-membered|      0.0|      0.0|\n",
      "|adj|        100|      0.0|      0.0|\n",
      "|adj|       1000|      0.0|      0.0|\n",
      "|adj|     1000th|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(19).show()\n",
    "# `show()` can also be chained to certain outputs like `limit`.\n",
    "# `show` by itself is a compound operation for displaying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why should you need to be careful when displaying data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so nice to use for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More DataFrame and Series operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform dataframe to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pos='adj', word='.22-caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22-calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.38-caliber', pos_score=0.0, neg_score=0.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from list of Row objects to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pos': 'adj', 'word': '.22-caliber', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.22-calibre', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.22_caliber', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.22_calibre', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.38-caliber', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.38-calibre', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.38_caliber', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.38_calibre', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.45-caliber', 'pos_score': 0.0, 'neg_score': 0.0},\n",
       " {'pos': 'adj', 'word': '.45-calibre', 'pos_score': 0.0, 'neg_score': 0.0}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.asDict() for row in df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting DataFrame Series\n",
    "\n",
    "Selecting variables with Spark **DataFrames API** works very similarly to Pandas DataFrames.  When selecting variables in Pandas use a `list` object, passed to a DataFrame object via `[]` brackets like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in spark is using the `.select()` method which takes flat parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pos: string, word: string, pos_score: double, neg_score: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|       word|pos_score|\n",
      "+-----------+---------+\n",
      "|.22-caliber|      0.0|\n",
      "|.22-calibre|      0.0|\n",
      "|.22_caliber|      0.0|\n",
      "|.22_calibre|      0.0|\n",
      "|.38-caliber|      0.0|\n",
      "|.38-calibre|      0.0|\n",
      "|.38_caliber|      0.0|\n",
      "|.38_calibre|      0.0|\n",
      "|.45-caliber|      0.0|\n",
      "|.45-calibre|      0.0|\n",
      "|.45_caliber|      0.0|\n",
      "|.45_calibre|      0.0|\n",
      "|          0|      0.0|\n",
      "|          1|      0.0|\n",
      "|         10|      0.0|\n",
      "|10-membered|      0.0|\n",
      "|        100|      0.0|\n",
      "|       1000|      0.0|\n",
      "|     1000th|      0.0|\n",
      "|      100th|      0.0|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"word\", \"pos_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"** and **\"col2\"** with\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|(pos_score + neg_score)|\n",
      "+-----------------------+\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.5|\n",
      "|                   0.25|\n",
      "|                    0.0|\n",
      "+-----------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"pos_score\"] + df[\"neg_score\"]).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|(pos_score + 10)|\n",
      "+----------------+\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "+----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also do math operations in series as well\n",
    "df.select(df[\"pos_score\"] + 10).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"alias\"\n",
    "As selections are keyed by conditions, they can become hard to read.  We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_score|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.5|\n",
      "|       0.25|\n",
      "|        0.0|\n",
      "+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select((df[\"pos_score\"] + df[\"neg_score\"]).alias(\"total_score\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+----------+\n",
      "|pos|       word|pos_score|neg_score|new_column|\n",
      "+---+-----------+---------+---------+----------+\n",
      "|adj|.22-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|          0|      0.0|      0.5|       0.5|\n",
      "|adj|          1|      0.0|     0.25|      0.25|\n",
      "|adj|         10|      0.0|      0.0|       0.0|\n",
      "+---+-----------+---------+---------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['pos_score'] + df['neg_score'])\n",
    "df.withColumn(\"new_column\", df['pos_score'] + df['neg_score']).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas we use \"masks\" through dataframe object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|      a-one|    0.625|      0.0|\n",
      "|adj|   abatable|    0.625|      0.0|\n",
      "|adj|abolishable|    0.625|      0.0|\n",
      "|adj|   absolved|    0.625|      0.0|\n",
      "|adj|        ace|    0.625|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"pos_score\"] > .5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple conditions\n",
    "SAME AS PANDAS!  Thankfully, we don't have to leave our comfort zone with too many oddities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"pos_score\"] > .5) & (df[\"neg_score\"] > 0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .filter(df[\"pos_score\"] > .5) \\\n",
    "    .filter(df[\"neg_score\"] > 0).show(5)  # Filters can be chained per line using the \\ newline escape sequence character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an expression\n",
    "Pandas has a similar function called \"where\".  However, with Spark `filter`, we can filter by shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|      a-one|    0.625|      0.0|\n",
      "|adj|   abatable|    0.625|      0.0|\n",
      "|adj|abolishable|    0.625|      0.0|\n",
      "|adj|   absolved|    0.625|      0.0|\n",
      "|adj|        ace|    0.625|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condition = \"\"\"\n",
    "pos_score > .5 AND \n",
    "neg_score > 0 \n",
    "\"\"\"\n",
    "\n",
    "df.filter(condition).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+---------+\n",
      "| pos|      word|pos_score|neg_score|\n",
      "+----+----------+---------+---------+\n",
      "| adj| cheapjack|      0.0|      1.0|\n",
      "| adj| henpecked|      0.0|      1.0|\n",
      "| adj|lamentable|      0.0|      1.0|\n",
      "|noun| angriness|      0.0|      1.0|\n",
      "|noun|blackguard|      0.0|      1.0|\n",
      "+----+----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.neg_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|  excellent|      1.0|      0.0|\n",
      "|adj| top-flight|      1.0|      0.0|\n",
      "|adj|fantabulous|      1.0|      0.0|\n",
      "|adj|first-class|      1.0|      0.0|\n",
      "|adj|  homologic|      1.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.pos_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------+---------+\n",
      "|pos|         word|pos_score|neg_score|\n",
      "+---+-------------+---------+---------+\n",
      "|adj|        awing|    0.875|    0.125|\n",
      "|adj|          fab|    0.875|    0.125|\n",
      "|adj|straightarrow|    0.875|    0.125|\n",
      "|adj|    gladdened|    0.875|    0.125|\n",
      "|adj|      awesome|    0.875|    0.125|\n",
      "+---+-------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.desc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------------+--------------+\n",
      "| pos|        word|     pos_score|     neg_score|\n",
      "+----+------------+--------------+--------------+\n",
      "| adj|   bona_fide|        0.5125|           0.3|\n",
      "| adj|   authentic|        0.5125|           0.3|\n",
      "| adj|      hearty|         0.525|          0.05|\n",
      "| adj|prophylactic|0.527666666667|0.222333333333|\n",
      "|noun|        joke|       0.53125|        0.1875|\n",
      "+----+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.asc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a histogram from a column\n",
    "\n",
    "We select a single column, transform it into an RDD, flatten it, and can then apply the RDD histogram function which returns arrays with bin-boundaries and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.1,\n",
       "  0.2,\n",
       "  0.30000000000000004,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.6000000000000001,\n",
       "  0.7000000000000001,\n",
       "  0.8,\n",
       "  0.9,\n",
       "  1.0],\n",
       " [131707, 8151, 5241, 3510, 418, 2831, 2029, 1109, 280, 11])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram = df.select('neg_score').rdd.flatMap(lambda x: x).histogram(10)\n",
    "histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 256,
       "width": 384
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(histogram[0][:-1], histogram[1], width=np.diff(histogram[0]), ec=\"k\", align=\"edge\")\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AS SQL!?\n",
    "\n",
    "\n",
    "Working with DataFrames and SQL is as easy as creating a **temporary view**. In this way you define the table name to refer to in SQL-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = spark.sql(\"SELECT * FROM sentiment LIMIT 100\")\n",
    "sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.toPandas()['neg_score'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views select DataFrames\n",
    "So the same transformations can be applied to DataFrames as we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+---------+-------------------+\n",
      "|summary| pos|              word|pos_score|          neg_score|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "|  count| 100|               100|      100|                100|\n",
      "|   mean|null|  96.9090909090909|      0.0|            0.02625|\n",
      "| stddev|null|154.37463012769066|      0.0|0.09282158132458183|\n",
      "|    min| adj|       .22-caliber|      0.0|                0.0|\n",
      "|    max| adj|              29th|      0.0|              0.625|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out another dataset using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load up the \"Pokemon\" basic Pokedex dataset\n",
    "First try without inferring the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\n",
    "    path=\"./data/pokedex_basic.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "|summary|PokedexNumber| Name| Type|Total|   HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|\n",
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "|  count|          800|  800|  800|  800|  800|   800|    800|          800|           800|  800|\n",
      "|   mean|        36...| null| null|43...|69...| 79...|  73...|        72.82|         71...|68...|\n",
      "|  st...|        20...| null| null|11...|25...| 32...|  31...|        32...|         27...|29...|\n",
      "|    min|            1|Ab...|  Bug|  180|    1|     5|      5|           10|            20|    5|\n",
      "|    max|          721|Zy...|Wa...|  780|  255|   190|    230|          194|           230|  180|\n",
      "+-------+-------------+-----+-----+-----+-----+------+-------+-------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check out the dataset with infer schema parameter but without header.\n",
    "How does it work with / without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PokedexNumber: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- SpecialAttack: integer (nullable = true)\n",
      " |-- SpecialDefense: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(PokedexNumber,IntegerType,true),StructField(Name,StringType,true),StructField(Type,StringType,true),StructField(Total,IntegerType,true),StructField(HP,IntegerType,true),StructField(Attack,IntegerType,true),StructField(Defense,IntegerType,true),StructField(SpecialAttack,IntegerType,true),StructField(SpecialDefense,IntegerType,true),StructField(Speed,IntegerType,true)))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|PokedexNumber|                Name|       Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "|            1|           Bulbasaur|GrassPoison|  318| 45|    49|     49|           65|            65|   45|\n",
      "|            2|             Ivysaur|GrassPoison|  405| 60|    62|     63|           80|            80|   60|\n",
      "|            3|            Venusaur|GrassPoison|  525| 80|    82|     83|          100|           100|   80|\n",
      "|            3|VenusaurMega Venu...|GrassPoison|  625| 80|   100|    123|          122|           120|   80|\n",
      "|            4|          Charmander|       Fire|  309| 39|    52|     43|           60|            50|   65|\n",
      "|            5|          Charmeleon|       Fire|  405| 58|    64|     58|           80|            65|   80|\n",
      "|            6|           Charizard| FireFlying|  534| 78|    84|     78|          109|            85|  100|\n",
      "|            6|CharizardMega Cha...| FireDragon|  634| 78|   130|    111|          130|            85|  100|\n",
      "|            6|CharizardMega Cha...| FireFlying|  634| 78|   104|     78|          159|           115|  100|\n",
      "|            7|            Squirtle|      Water|  314| 44|    48|     65|           50|            64|   43|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.createOrReplaceTempView(\"pokemon\")\n",
    "sqlContext.sql(\"SELECT * FROM pokemon LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a Which is the strongest Pokemon by `Type`?\n",
    "Using Spark DataFrame operations.  Research Spark's \"groupBy\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|           Type|max(attack)|\n",
      "+---------------+-----------+\n",
      "|PsychicFighting|        190|\n",
      "|    BugFighting|        185|\n",
      "|        Psychic|        180|\n",
      "|   DragonFlying|        180|\n",
      "|     GroundFire|        180|\n",
      "+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df = df.select('Type', 'attack').groupBy(\n",
    "    'Type').max().sort('max(attack)', ascending=False)\n",
    "count_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_df.withColumn('attack', count_df['max(attack)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------+\n",
      "|                Name|           Type|attack|\n",
      "+--------------------+---------------+------+\n",
      "| MewtwoMega Mewtwo X|PsychicFighting|   190|\n",
      "|HeracrossMega Her...|    BugFighting|   185|\n",
      "|GroudonPrimal Gro...|     GroundFire|   180|\n",
      "|RayquazaMega Rayq...|   DragonFlying|   180|\n",
      "|  DeoxysAttack Forme|        Psychic|   180|\n",
      "|  KyuremBlack Kyurem|      DragonIce|   170|\n",
      "|GarchompMega Garc...|   DragonGround|   170|\n",
      "| BanetteMega Banette|          Ghost|   165|\n",
      "|           Rampardos|           Rock|   165|\n",
      "|TyranitarMega Tyr...|       RockDark|   164|\n",
      "|             Slaking|         Normal|   160|\n",
      "|           Regigigas|         Normal|   160|\n",
      "| DiancieMega Diancie|      RockFairy|   160|\n",
      "|  HoopaHoopa Unbound|    PsychicDark|   160|\n",
      "|BlazikenMega Blaz...|   FireFighting|   160|\n",
      "|   PinsirMega Pinsir|      BugFlying|   155|\n",
      "|GyaradosMega Gyar...|      WaterDark|   155|\n",
      "|SwampertMega Swam...|    WaterGround|   150|\n",
      "|   ScizorMega Scizor|       BugSteel|   150|\n",
      "|BeedrillMega Beed...|      BugPoison|   150|\n",
      "+--------------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df.join(df, on=['Type', 'attack']).select(\n",
    "    'Name', 'Type', 'attack').sort('attack', ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Which is the strongest Pokemon by Type?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+\n",
      "|                Name|           Type|TotalMax|\n",
      "+--------------------+---------------+--------+\n",
      "| MewtwoMega Mewtwo X|PsychicFighting|     190|\n",
      "|HeracrossMega Her...|    BugFighting|     185|\n",
      "|GroudonPrimal Gro...|     GroundFire|     180|\n",
      "|  DeoxysAttack Forme|        Psychic|     180|\n",
      "|RayquazaMega Rayq...|   DragonFlying|     180|\n",
      "|GarchompMega Garc...|   DragonGround|     170|\n",
      "|  KyuremBlack Kyurem|      DragonIce|     170|\n",
      "|           Rampardos|           Rock|     165|\n",
      "| BanetteMega Banette|          Ghost|     165|\n",
      "|TyranitarMega Tyr...|       RockDark|     164|\n",
      "+--------------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.Name, tbl.Type, tbl.TotalMax\n",
    "FROM (\n",
    "    SELECT Type, MAX(attack) AS TotalMax\n",
    "    FROM pokemon\n",
    "    GROUP BY Type\n",
    ") AS tbl \n",
    "LEFT OUTER JOIN pokemon p \n",
    "ON tbl.Type = p.Type AND tbl.TotalMax = p.attack\n",
    "ORDER BY TotalMax DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "strongest_pokemon = sqlContext.sql(sql)\n",
    "strongest_pokemon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Which Pokemon has the best combined Attack and Defence?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|                Name|       Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Poketotal|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          306|   AggronMega Aggron|      Steel|  630| 70|   140|    230|           60|            80|   50|      370|\n",
      "|          208| SteelixMega Steelix|SteelGround|  610| 75|   125|    230|           55|            95|   30|      355|\n",
      "|          383|GroudonPrimal Gro...| GroundFire|  770|100|   180|    160|          150|            90|   90|      340|\n",
      "|          248|TyranitarMega Tyr...|   RockDark|  700|100|   164|    150|           95|           120|   71|      314|\n",
      "|          713|             Avalugg|        Ice|  514| 95|   117|    184|           44|            46|   28|      301|\n",
      "+-------------+--------------------+-----------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df.withColumn(\"Poketotal\", df[\"Attack\"] + df[\"Defense\"]\n",
    "              ).sort(\"Poketotal\", ascending=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Which Pokemon has the best combined Attack and Defence?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|                Name|           Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Poketotal|\n",
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          306|   AggronMega Aggron|          Steel|  630| 70|   140|    230|           60|            80|   50|      370|\n",
      "|          208| SteelixMega Steelix|    SteelGround|  610| 75|   125|    230|           55|            95|   30|      355|\n",
      "|          383|GroudonPrimal Gro...|     GroundFire|  770|100|   180|    160|          150|            90|   90|      340|\n",
      "|          248|TyranitarMega Tyr...|       RockDark|  700|100|   164|    150|           95|           120|   71|      314|\n",
      "|          713|             Avalugg|            Ice|  514| 95|   117|    184|           44|            46|   28|      301|\n",
      "|          377|            Regirock|           Rock|  580| 80|   100|    200|           50|           100|   50|      300|\n",
      "|          214|HeracrossMega Her...|    BugFighting|  600| 80|   185|    115|           40|           105|   75|      300|\n",
      "|          376|MetagrossMega Met...|   SteelPsychic|  700| 80|   145|    150|          105|           110|  110|      295|\n",
      "|          212|   ScizorMega Scizor|       BugSteel|  600| 70|   150|    140|           65|           100|   75|      290|\n",
      "|          150| MewtwoMega Mewtwo X|PsychicFighting|  780|106|   190|    100|          154|           100|  130|      290|\n",
      "+-------------+--------------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, p.Attack + p.Defense AS Poketotal\n",
    "FROM pokemon p\n",
    "ORDER BY Poketotal DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|PokedexNumber|           Name|           Type|Total| HP|Attack|Defense|SpecialAttack|SpecialDefense|Speed|Pokevalue|\n",
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "|          289|        Slaking|         Normal|  670|150|   160|    100|           95|            65|  100|   7800.0|\n",
      "|          383|GroudonPrima...|     GroundFire|  770|100|   180|    160|          150|            90|   90|   6800.0|\n",
      "|          646|KyuremBlack ...|      DragonIce|  700|125|   170|    100|          120|            90|   95|   6750.0|\n",
      "|          487|GiratinaAlte...|    GhostDragon|  680|150|   100|    120|          100|           120|   90|   6600.0|\n",
      "|          487|GiratinaOrig...|    GhostDragon|  680|150|   120|    100|          120|           100|   90|   6600.0|\n",
      "|          248|TyranitarMeg...|       RockDark|  700|100|   164|    150|           95|           120|   71|   6280.0|\n",
      "|          464|      Rhyperior|     GroundRock|  535|115|   140|    130|           55|            55|   40|   6210.0|\n",
      "|          445|GarchompMega...|   DragonGround|  700|108|   170|    115|          120|            95|   92|   6156.0|\n",
      "|          150|MewtwoMega M...|PsychicFighting|  780|106|   190|    100|          154|           100|  130|   6148.0|\n",
      "|          486|      Regigigas|         Normal|  670|110|   160|    110|           80|           110|  100|   5940.0|\n",
      "+-------------+---------------+---------------+-----+---+------+-------+-------------+--------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, (p.Attack + p.Defense) * (p.HP * .2) AS Pokevalue\n",
    "FROM pokemon p\n",
    "ORDER BY Pokevalue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(truncate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "579px",
    "left": "561.111px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
