{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to `statsmodels`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "### Core\n",
    "- Understand what the statsmodels module is used for\n",
    "- Learn how to build a linear/logistic regression model using statsmodels \n",
    "- Understand the practical differences between scikit-learn and statsmodels\n",
    "- Interpret the output of models from statsmodels\n",
    "\n",
    "### Target\n",
    "- Know how to create formulas using the patsy module to easily specify target and predictor matrices\n",
    "- Use the statsmodels formula api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statsmodels is another much used package for statistical modeling and machine learning. However, whereas scikit-learn focuses more on predictions, for statsmodels the main focus is on the statistics and inference side. It contains a lower variety of models, but instead is able to run a number of frequentist statistical hypothesis tests and to determine confidence intervals to get insight about the validity of a model.\n",
    "\n",
    "[The statsmodels documentation can be found here.](http://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094095\n",
      "36.459488385089855\n",
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(lr.score(X, y))\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)\n",
    "\n",
    "# this is how we did this in sklearn, whereas... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a linear regression using `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "Now we will fit the linear regression model using `statsmodels`.\n",
    "\n",
    "First we load the statsmodels api module, which contains the ordinary least squares `OLS` model class. The statsmodels process is slightly different:\n",
    "- We manually make a new column for the intercept in our design matrix $X$.\n",
    "- The $y$ target variable comes before the $X$ predictor\n",
    "- The data is provided during the instantiation of the model object, then fit is called without the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Noah/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    1.0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    1.0  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    1.0  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    1.0  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    1.0  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding the intercept manually\n",
    "sm.add_constant(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score: 0.7406426641094095\n",
      "RMSE: 4.679191295697281\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X)) # statsmodel version of sklearn's LinearRegression\n",
    "results = model.fit()\n",
    "predictions = results.predict(sm.add_constant(X))\n",
    "print(\"R2-score:\", results.rsquared)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels model coefficients\n",
    "\n",
    "The model coefficients (coefficients and intercept) can be extract from the fitted model with `.params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      36.459488\n",
       "CRIM       -0.108011\n",
       "ZN          0.046420\n",
       "INDUS       0.020559\n",
       "CHAS        2.686734\n",
       "NOX       -17.766611\n",
       "RM          3.809865\n",
       "AGE         0.000692\n",
       "DIS        -1.475567\n",
       "RAD         0.306049\n",
       "TAX        -0.012335\n",
       "PTRATIO    -0.952747\n",
       "B           0.009312\n",
       "LSTAT      -0.524758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that results for intercept and coefficients agree in both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(lr.intercept_, results.params.const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(lr.coef_, results.params[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels `.summary()`  function\n",
    "\n",
    "Once a model is fit with statsmodels, you can print out a variety of summary statistics, metrics, and properties of the model using the `model.summary()` function.\n",
    "\n",
    "You are already familiar with some of the information available in the summary:\n",
    "- R-squared\n",
    "- Number of observations\n",
    "- Coefficients for the variables and the intercept (const)\n",
    "- Standard errors of the coefficients, t-statistics, p-values, and confidence intervals\n",
    "\n",
    "There is also a variety of different metrics that we have not yet talked about. Don't hesitate to look up any of the statistics online if you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 02 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>6.72e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:41:51</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4595</td> <td>    5.103</td> <td>    7.144</td> <td> 0.000</td> <td>   26.432</td> <td>   46.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.1080</td> <td>    0.033</td> <td>   -3.287</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.382</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>    0.0206</td> <td>    0.061</td> <td>    0.334</td> <td> 0.738</td> <td>   -0.100</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.6867</td> <td>    0.862</td> <td>    3.118</td> <td> 0.002</td> <td>    0.994</td> <td>    4.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>  -17.7666</td> <td>    3.820</td> <td>   -4.651</td> <td> 0.000</td> <td>  -25.272</td> <td>  -10.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    3.8099</td> <td>    0.418</td> <td>    9.116</td> <td> 0.000</td> <td>    2.989</td> <td>    4.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>    0.0007</td> <td>    0.013</td> <td>    0.052</td> <td> 0.958</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -1.4756</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.867</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.3060</td> <td>    0.066</td> <td>    4.613</td> <td> 0.000</td> <td>    0.176</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.280</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.9527</td> <td>    0.131</td> <td>   -7.283</td> <td> 0.000</td> <td>   -1.210</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0093</td> <td>    0.003</td> <td>    3.467</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.5248</td> <td>    0.051</td> <td>  -10.347</td> <td> 0.000</td> <td>   -0.624</td> <td>   -0.425</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.041</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 783.126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>8.84e-171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.281</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.51e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Sun, 02 Feb 2020   Prob (F-statistic):          6.72e-135\n",
       "Time:                        16:41:51   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4595      5.103      7.144      0.000      26.432      46.487\n",
       "CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n",
       "ZN             0.0464      0.014      3.382      0.001       0.019       0.073\n",
       "INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141\n",
       "CHAS           2.6867      0.862      3.118      0.002       0.994       4.380\n",
       "NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n",
       "RM             3.8099      0.418      9.116      0.000       2.989       4.631\n",
       "AGE            0.0007      0.013      0.052      0.958      -0.025       0.027\n",
       "DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n",
       "RAD            0.3060      0.066      4.613      0.000       0.176       0.436\n",
       "TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n",
       "PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n",
       "B              0.0093      0.003      3.467      0.001       0.004       0.015\n",
       "LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n",
       "==============================================================================\n",
       "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
       "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
       "Kurtosis:                       8.281   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to `patsy` formulas\n",
    "\n",
    "---\n",
    "\n",
    "Why slice and dice the data yourself when you just write a formula that defines your model?\n",
    "\n",
    "The `patsy` package allows you to specify the construction of your model using a formula string, and then returns the matrices required to fit the model.\n",
    "\n",
    "Let's say we wanted to predict `CRIM` from `TAX`, `AGE` and `ZN`. We would write a string formula like\n",
    "\n",
    "```\n",
    "formula = 'CRIM ~ TAX + AGE + ZN'\n",
    "```\n",
    "\n",
    "Then, after importing patsy, we can generate our target and predictor matrix by supplying the formula and the dataframe that contains the corresponding columns.\n",
    "\n",
    "```python\n",
    "import patsy\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df['MEDV'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "formula = 'MEDV ~ TAX + AGE + ZN'\n",
    "\n",
    "y, X_subset = patsy.dmatrices(formula, data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MEDV\n",
       "0  24.0\n",
       "1  21.6\n",
       "2  34.7\n",
       "3  33.4\n",
       "4  36.2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>TAX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ZN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>78.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>61.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept    TAX   AGE    ZN\n",
       "0        1.0  296.0  65.2  18.0\n",
       "1        1.0  242.0  78.9   0.0\n",
       "2        1.0  242.0  61.1   0.0\n",
       "3        1.0  222.0  45.8   0.0\n",
       "4        1.0  222.0  54.2   0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that with `return_type='dataframe'` patsy's `.dmatrices()` function returns two pandas dataframes, one for the target and one for the design matrix. \n",
    "\n",
    "You'll also notice that it creates an intercept column by default. **If you do not want it to create an intercept column, add a -1 to the formula string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ZN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>242.0</td>\n",
       "      <td>78.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>242.0</td>\n",
       "      <td>61.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>222.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TAX   AGE    ZN\n",
       "0  296.0  65.2  18.0\n",
       "1  242.0  78.9   0.0\n",
       "2  242.0  61.1   0.0\n",
       "3  222.0  45.8   0.0\n",
       "4  222.0  54.2   0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = 'MEDV ~ TAX + AGE + ZN -1'\n",
    "\n",
    "y, X_subset = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "\n",
    "X_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the matrices obtained from patsy in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'MEDV ~ TAX + AGE + ZN'\n",
    "\n",
    "y, X_subset = patsy.dmatrices(formula, data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   62.89</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 02 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.56e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:41:51</td>     <th>  Log-Likelihood:    </th> <td> -1759.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3527.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   502</td>      <th>  BIC:               </th> <td>   3544.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   31.5071</td> <td>    1.322</td> <td>   23.828</td> <td> 0.000</td> <td>   28.909</td> <td>   34.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>       <td>   -0.0200</td> <td>    0.002</td> <td>   -8.296</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>   -0.0250</td> <td>    0.017</td> <td>   -1.503</td> <td> 0.134</td> <td>   -0.058</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>        <td>    0.0795</td> <td>    0.018</td> <td>    4.351</td> <td> 0.000</td> <td>    0.044</td> <td>    0.115</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>203.569</td> <th>  Durbin-Watson:     </th> <td>   0.685</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 735.075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.870</td>  <th>  Prob(JB):          </th> <td>2.40e-160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.568</td>  <th>  Cond. No.          </th> <td>1.69e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.69e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.273\n",
       "Model:                            OLS   Adj. R-squared:                  0.269\n",
       "Method:                 Least Squares   F-statistic:                     62.89\n",
       "Date:                Sun, 02 Feb 2020   Prob (F-statistic):           1.56e-34\n",
       "Time:                        16:41:51   Log-Likelihood:                -1759.5\n",
       "No. Observations:                 506   AIC:                             3527.\n",
       "Df Residuals:                     502   BIC:                             3544.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     31.5071      1.322     23.828      0.000      28.909      34.105\n",
       "TAX           -0.0200      0.002     -8.296      0.000      -0.025      -0.015\n",
       "AGE           -0.0250      0.017     -1.503      0.134      -0.058       0.008\n",
       "ZN             0.0795      0.018      4.351      0.000       0.044       0.115\n",
       "==============================================================================\n",
       "Omnibus:                      203.569   Durbin-Watson:                   0.685\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              735.075\n",
       "Skew:                           1.870   Prob(JB):                    2.40e-160\n",
       "Kurtosis:                       7.568   Cond. No.                     1.69e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.69e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y, X_subset)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even construct more complicated formulas. For example it is possible to use numpy-functions inside the formula language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>TAX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>TAX:AGE</th>\n",
       "      <th>TAX:CRIM</th>\n",
       "      <th>AGE:CRIM</th>\n",
       "      <th>np.power(AGE, 2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>19299.2</td>\n",
       "      <td>1.87072</td>\n",
       "      <td>0.412064</td>\n",
       "      <td>4251.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>78.9</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>19093.8</td>\n",
       "      <td>6.60902</td>\n",
       "      <td>2.154759</td>\n",
       "      <td>6225.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>61.1</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>14786.2</td>\n",
       "      <td>6.60418</td>\n",
       "      <td>1.667419</td>\n",
       "      <td>3733.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>10167.6</td>\n",
       "      <td>7.18614</td>\n",
       "      <td>1.482546</td>\n",
       "      <td>2097.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>12032.4</td>\n",
       "      <td>15.32910</td>\n",
       "      <td>3.742510</td>\n",
       "      <td>2937.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept    TAX   AGE     CRIM  TAX:AGE  TAX:CRIM  AGE:CRIM  \\\n",
       "0        1.0  296.0  65.2  0.00632  19299.2   1.87072  0.412064   \n",
       "1        1.0  242.0  78.9  0.02731  19093.8   6.60902  2.154759   \n",
       "2        1.0  242.0  61.1  0.02729  14786.2   6.60418  1.667419   \n",
       "3        1.0  222.0  45.8  0.03237  10167.6   7.18614  1.482546   \n",
       "4        1.0  222.0  54.2  0.06905  12032.4  15.32910  3.742510   \n",
       "\n",
       "   np.power(AGE, 2)  \n",
       "0           4251.04  \n",
       "1           6225.21  \n",
       "2           3733.21  \n",
       "3           2097.64  \n",
       "4           2937.64  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = 'MEDV ~ (TAX + AGE + CRIM)**2 + np.power(AGE, 2)'\n",
    "\n",
    "y, X_formula = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "X_formula.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels formula api\n",
    "\n",
    "Statsmodels has the option to use formula expressions straightforwardly.\n",
    "To do so, one has to\n",
    "\n",
    "- import a different submodule\n",
    "- indicate a dataframe including the target variable\n",
    "- use lower case for `ols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# this different sub-module and therefore the different (lowercase) ols we use below can handle the patsy\n",
    "# formulas, clue is in the name of the submodule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('+').join(X.columns)\n",
    "# good thing about patsy you can join many many columns in an automated way, rather than manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score:  0.2731750019949428\n",
      "RMSE: 7.833150307174811\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   62.89</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 05 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.56e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:30:11</td>     <th>  Log-Likelihood:    </th> <td> -1759.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3527.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   502</td>      <th>  BIC:               </th> <td>   3544.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   31.5071</td> <td>    1.322</td> <td>   23.828</td> <td> 0.000</td> <td>   28.909</td> <td>   34.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>       <td>   -0.0200</td> <td>    0.002</td> <td>   -8.296</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>   -0.0250</td> <td>    0.017</td> <td>   -1.503</td> <td> 0.134</td> <td>   -0.058</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>        <td>    0.0795</td> <td>    0.018</td> <td>    4.351</td> <td> 0.000</td> <td>    0.044</td> <td>    0.115</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>203.569</td> <th>  Durbin-Watson:     </th> <td>   0.685</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 735.075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.870</td>  <th>  Prob(JB):          </th> <td>2.40e-160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.568</td>  <th>  Cond. No.          </th> <td>1.69e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.69e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.273\n",
       "Model:                            OLS   Adj. R-squared:                  0.269\n",
       "Method:                 Least Squares   F-statistic:                     62.89\n",
       "Date:                Wed, 05 Feb 2020   Prob (F-statistic):           1.56e-34\n",
       "Time:                        12:30:11   Log-Likelihood:                -1759.5\n",
       "No. Observations:                 506   AIC:                             3527.\n",
       "Df Residuals:                     502   BIC:                             3544.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     31.5071      1.322     23.828      0.000      28.909      34.105\n",
       "TAX           -0.0200      0.002     -8.296      0.000      -0.025      -0.015\n",
       "AGE           -0.0250      0.017     -1.503      0.134      -0.058       0.008\n",
       "ZN             0.0795      0.018      4.351      0.000       0.044       0.115\n",
       "==============================================================================\n",
       "Omnibus:                      203.569   Durbin-Watson:                   0.685\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              735.075\n",
       "Skew:                           1.870   Prob(JB):                    2.40e-160\n",
       "Kurtosis:                       7.568   Cond. No.                     1.69e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.69e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.ols(formula='MEDV ~ TAX + AGE + ZN', data=df)\n",
    "results = model.fit()\n",
    "predictions = results.predict(X)\n",
    "print(\"R2-score: \", results.rsquared)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y, predictions)))\n",
    "results.summary()\n",
    "\n",
    "# the P column below shows the importance of that variable to the model. Greater than 0 means it can be removed\n",
    "# but then you'd have to remove and recalc the P-values.\n",
    "# P here is null hypothesis test.. small means reject null hypothesis, large means accept that coefficient is \n",
    "# helpful to model. Note these are probability calcs, so within confidence you are making right choice, but not\n",
    "# guaranteed, and particularly if you do this repeatedly, this chance of removing the wrong variable increases\n",
    "\n",
    "# the last table below is about residuals, which need to be normally distributed for the CI for coefficients to\n",
    "# be believable, we can see here they're not normally distributed\n",
    "\n",
    "# Durbin-Watson tells if there's some pattern in the residuals. e.g. if residuals are smaller for more recent\n",
    "# datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2\n",
    "\n",
    "This gives the usual R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2731750019949428"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV    0.273175\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(results.resid**2)/y.var(ddof=0)\n",
    "# unlike in sklearn, statsmodels stores the resid so you don't have to manually calc like we did in sklearn\n",
    "# here we demonstrate using it to manually prove R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 adjusted\n",
    "\n",
    "The adjusted R2 score is sometimes used for model selection. It depends on the number of parameters in the model in such a way that with too many parameters it would become worse.\n",
    "\n",
    "This is a good way to cross validate, or score on test data, when you don't want to refit and score that way.\n",
    "\n",
    "You can just use R2_adj to replicate the action of overfitting to too many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2688314263096536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV    0.268831\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.sum(results.resid**2) / (results.df_resid) / y.var(ddof=1)\n",
    "# ddof is the 1 you subtract from N denom in var calc, which is what you need to do if it's the sample variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.df_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-value\n",
    "\n",
    "The F-value is related to a statistical test with the null hypothesis that all model coefficients are zero and the alternative hypothesis that the included model coefficients should be non-zero. A low p-value suggests that the model is better than baseline.\n",
    "\n",
    "ie. is it better than baseline... ie is RSS less than TSS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.891732937941306"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.fvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5571448678263912e-34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.f_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Log-Likelihood\n",
    "\n",
    "The maximum log-likelihood value is derived from the joint probability of the observed residuals. A higher value (the sign does matter) would speak for a better model.\n",
    "\n",
    "Basically like the RSS where we wanted model to minimize it, here llf is same but just we want to maximise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1759.5154699883312"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.llf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC and BIC\n",
    "\n",
    "The AIC (Akaike's information criterion) and BIC (Bayesian information criterion) are measures for how well a model can be expected to work on unseen data. They take the maximum likelihood value, the number of model parameters, and, in the case of BIC, the number of observations into account and return a single number.\n",
    "\n",
    "For both scores, lower is better and models can be ranked accordingly. For linear regression, they can be calculated through simple formulas. AIC can be regarded as an approximation of what one should obtain from leave-one-out cross validation. Both of the scores stem from times where carrying out cross validation was too costly computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3527.0309399766625"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3527.0309399766625"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(results.df_model+1)-2*results.llf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543.9370866538125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543.9370866538125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * np.log(results.nobs) * (results.df_model-1) - 2 * results.llf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals\n",
    "\n",
    "The estimated standard errors can be used to calculate confidence intervals for the model coefficients. The desired level of confidence can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>28.909253</td>\n",
       "      <td>34.104966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.024725</td>\n",
       "      <td>-0.015256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>-0.057772</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.043599</td>\n",
       "      <td>0.115380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1\n",
       "Intercept  28.909253  34.104966\n",
       "TAX        -0.024725  -0.015256\n",
       "AGE        -0.057772   0.007700\n",
       "ZN          0.043599   0.115380"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test values\n",
    "\n",
    "This is a test for the model having the single factor inside or not. A low p-value suggests that the predictor should be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    23.828122\n",
       "TAX          -8.295541\n",
       "AGE          -1.502585\n",
       "ZN            4.351384\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    1.629113e-84\n",
       "TAX          1.001851e-15\n",
       "AGE          1.335750e-01\n",
       "ZN           1.639719e-05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results about residuals\n",
    "\n",
    "The lower part of the summary table contains results about the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jb': 735.0749086264958,\n",
       " 'jbpv': 2.4016609627535522e-160,\n",
       " 'skew': 1.8704754997113586,\n",
       " 'kurtosis': 7.568424091226796,\n",
       " 'omni': 203.56853895680757,\n",
       " 'omnipv': 6.246739866698115e-45,\n",
       " 'condno': 1690.7688402622623,\n",
       " 'mineigval': 35.36901535068985}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.diagn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omnibus test\n",
    "\n",
    "This is the D'Agostino-Pearson normality test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormaltestResult(statistic=203.56853895680757, pvalue=6.246739866698115e-45)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.normaltest(results.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 256,
       "width": 386
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.resid.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness and kurtosis of residuals\n",
    "\n",
    "If the residuals were normally distributed, they should not have any skew and the kurtosis should be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8704754997113586, 7.568424091226796)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.skew(results.resid), stats.kurtosis(results.resid, fisher=False, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarque-Bera test\n",
    "\n",
    "This is another normality test. This test should only be used with enough data samples (more than 2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735.074908626496, 0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.jarque_bera(results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson test\n",
    "\n",
    "This tests for serial autocorrelations among the residuals.\n",
    "Values close to 0 imply positive serial autocorrelation, values close to 2 imply absence of serial autocorrelation, values close to 4 imply negative serial autocorrelation. Serial autocorrelation could be observed if you data recorded over time and knowing on which day it is recorded helps you to estimate how large the residuals are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849357296824539"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.stats.durbin_watson(results.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849357296824539"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((results.resid - results.resid.shift(1)).dropna()**2).sum() / (results.resid**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning number\n",
    "\n",
    "The conditioning number is a measure of multi-collinearity in the predictor matrix. You might see statsmodels giving warnings about large conditioning numbers. It is related to the relative sizes of the largest and smallest eigenvalues of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690.7688402622623"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.condition_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting with regularization\n",
    "\n",
    "We can use regularization in form of an elastic net, allowing to reduce the model to either Lasso or Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_reg = model.fit_regularized(alpha=1, L1_wt=0.5, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff58218f1b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_reg' is not defined"
     ]
    }
   ],
   "source": [
    "results_reg.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is only implemented for binary classification.\n",
    "Note that by default no regularization is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X.columns = [col.replace(' ', '_') for col in X.columns]\n",
    "X['target'] = data.target\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
       "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
       "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
       "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
       "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
       "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
       "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
       "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
       "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['mean_radius', 'mean_texture', 'mean_perimeter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192353\n",
      "         Iterations 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crahmede/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>   569</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   565</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 02 Feb 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.7087</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:41:52</td>     <th>  Log-Likelihood:    </th>  <td> -109.45</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -375.72</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.225e-115</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>          <td>   18.3255</td> <td>    1.961</td> <td>    9.347</td> <td> 0.000</td> <td>   14.483</td> <td>   22.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_radius</th>    <td>    6.0262</td> <td>    0.996</td> <td>    6.048</td> <td> 0.000</td> <td>    4.073</td> <td>    7.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_texture</th>   <td>   -0.2428</td> <td>    0.045</td> <td>   -5.342</td> <td> 0.000</td> <td>   -0.332</td> <td>   -0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_perimeter</th> <td>   -1.0675</td> <td>    0.155</td> <td>   -6.889</td> <td> 0.000</td> <td>   -1.371</td> <td>   -0.764</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  569\n",
       "Model:                          Logit   Df Residuals:                      565\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Sun, 02 Feb 2020   Pseudo R-squ.:                  0.7087\n",
       "Time:                        16:41:52   Log-Likelihood:                -109.45\n",
       "converged:                       True   LL-Null:                       -375.72\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.225e-115\n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "const             18.3255      1.961      9.347      0.000      14.483      22.168\n",
       "mean_radius        6.0262      0.996      6.048      0.000       4.073       7.979\n",
       "mean_texture      -0.2428      0.045     -5.342      0.000      -0.332      -0.154\n",
       "mean_perimeter    -1.0675      0.155     -6.889      0.000      -1.371      -0.764\n",
       "==================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.Logit(y, sm.add_constant(X[columns]))\n",
    "results = model.fit()\n",
    "results.summary()\n",
    "\n",
    "# the LLR p-value is a little like the F-statistic where you remove all coefficients to 0, more drastic than\n",
    "# the individual tests in the P>|z| column. Though you can expect all P's to be > 0 if LLR p is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With formula\n",
    "\n",
    "This won't like spaces in the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'target ~ ' + ('+').join(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target ~ mean_radius+mean_texture+mean_perimeter'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192353\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>   <td>   569</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   565</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 02 Feb 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.7087</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:41:52</td>     <th>  Log-Likelihood:    </th>  <td> -109.45</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -375.72</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.225e-115</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>   18.3255</td> <td>    1.961</td> <td>    9.347</td> <td> 0.000</td> <td>   14.483</td> <td>   22.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_radius</th>    <td>    6.0262</td> <td>    0.996</td> <td>    6.048</td> <td> 0.000</td> <td>    4.073</td> <td>    7.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_texture</th>   <td>   -0.2428</td> <td>    0.045</td> <td>   -5.342</td> <td> 0.000</td> <td>   -0.332</td> <td>   -0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_perimeter</th> <td>   -1.0675</td> <td>    0.155</td> <td>   -6.889</td> <td> 0.000</td> <td>   -1.371</td> <td>   -0.764</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                  569\n",
       "Model:                          Logit   Df Residuals:                      565\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Sun, 02 Feb 2020   Pseudo R-squ.:                  0.7087\n",
       "Time:                        16:41:52   Log-Likelihood:                -109.45\n",
       "converged:                       True   LL-Null:                       -375.72\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.225e-115\n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept         18.3255      1.961      9.347      0.000      14.483      22.168\n",
       "mean_radius        6.0262      0.996      6.048      0.000       4.073       7.979\n",
       "mean_texture      -0.2428      0.045     -5.342      0.000      -0.332      -0.154\n",
       "mean_perimeter    -1.0675      0.155     -6.889      0.000      -1.371      -0.764\n",
       "==================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.logit(formula, data=X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted probabilities for class 1\n",
    "\n",
    "With the predict method we obtain the predicted probabilities for being part of class 1, not the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00103359, 0.01980182, 0.00095381, 0.34966246, 0.00082017,\n",
       "       0.80150452, 0.01446631, 0.40088603, 0.11522583, 0.11272487])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted probabilities\n",
    "results.predict()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted labels ... need to do this manually in statsmodels, whereas sklearn you could've got it automated.\n",
    "# note - statsmodels only calls them 0,1 and only binary classification not multi_class\n",
    "threshold = 0.5\n",
    "y_hat = (results.predict() > threshold) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191564147627417"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy at threshold\n",
    "(y == y_hat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[184.,  28.],\n",
       "       [ 18., 339.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix at threshold\n",
    "results.pred_table(threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum log-likelihood is obtained by forming the sum of the predicted log-probabilities depending on which class the observations are in.\n",
    "\n",
    "This is what the regression is trying to optimise by making as large as possible (ie, less negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-109.44873049463472"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember pred is the prob(class 1), so if it was indeed y_true = 1 and model was correctly giving a high \n",
    "# prob(class 1) then np.log(pred) would be close to 0, good.\n",
    "\n",
    "# so there's two ways it could go wrong, ie. large, -ve Max Log-Likelihood:\n",
    "# 1. you predicted correctly, but actually the pred.prob was close to threshold, therefore overlapped classes\n",
    "# although in practise this effect will be swamped by:...\n",
    "# 2. you predict incorrectly, with great certainty, in that case we take np.log(1-pred) which will be v. -ve\n",
    "\n",
    "sum([np.log(pred) \n",
    "     if y_true==1 else np.log(1-pred) \n",
    "     for y_true, pred in zip(y, results.predict())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-109.44873049463473"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.llf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just predict the majority class, we obtain the likelihood for the null model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-375.72000269208684"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([np.log(y.mean()) \n",
    "     if y_true==1 else np.log(1-y.mean()) \n",
    "     for y_true in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-375.7200027320281"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.llnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood ratio test assesses if the proposed model is better than the baseline model. A low p-value indicates that it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.224606088286518e-115"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.llr_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo R-squared\n",
    "\n",
    "The pseudo r-squared measures how much the model is outperforming the baseline in a similar way to the R2 in regression. It is hardly used for model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pseudo r-squared at statsgeek](https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7086960244363247"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.prsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7086960244363247"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - results.llf / results.llnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "Also these are derived from the standard error estimates and the desired confidence level can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>14.482743</td>\n",
       "      <td>22.168304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_radius</th>\n",
       "      <td>4.073359</td>\n",
       "      <td>7.979103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_texture</th>\n",
       "      <td>-0.331841</td>\n",
       "      <td>-0.153690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_perimeter</th>\n",
       "      <td>-1.371249</td>\n",
       "      <td>-0.763787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0          1\n",
       "Intercept       14.482743  22.168304\n",
       "mean_radius      4.073359   7.979103\n",
       "mean_texture    -0.331841  -0.153690\n",
       "mean_perimeter  -1.371249  -0.763787"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values\n",
    "\n",
    "The p-values are from testing if the model is performing better with or without the predictor while keeping all the others. A low p-value suggests that the predictor should be part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept         9.346713\n",
       "mean_radius       6.048115\n",
       "mean_texture     -5.341667\n",
       "mean_perimeter   -6.888650\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept         9.041382e-21\n",
       "mean_radius       1.465505e-09\n",
       "mean_texture      9.209554e-08\n",
       "mean_perimeter    5.632436e-12\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Statsmodels gives much more information about model performance than sklearn. The emphasis is on inference rather than on prediction. For default models you can obtain confidence intervals for model coefficients. Regularization is possible, but then often no confidence intervals can be calculated. There is no direct cross validation implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    " \n",
    "\n",
    "- [Statsmodels documentation](http://www.statsmodels.org/stable/index.html)\n",
    "- http://efavdb.com/interpret-linear-regression/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
