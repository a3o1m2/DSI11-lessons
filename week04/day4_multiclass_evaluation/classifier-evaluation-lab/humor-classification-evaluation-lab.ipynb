{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Evaluating Classification Models on Humor Styles Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be practicing evaluating classification models (Logistic Regression in particular) on a \"Humor Styles\" survey.\n",
    "\n",
    "This survey is designed to evaluate what \"style\" of humor subjects have. Your goal will be to classify gender using the responses on the survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humor styles questions encoding reference\n",
    "\n",
    "### 32 questions:\n",
    "\n",
    "Subjects answered **32** different questions outlined below:\n",
    "\n",
    "    1. I usually don't laugh or joke with other people.\n",
    "    2. If I feel depressed, I can cheer myself up with humor.\n",
    "    3. If someone makes a mistake, I will tease them about it.\n",
    "    4. I let people laugh at me or make fun of me at my expense more than I should.\n",
    "    5. I don't have to work very hard to make other people laugh. I am a naturally humorous person.\n",
    "    6. Even when I'm alone, I am often amused by the absurdities of life.\n",
    "    7. People are never offended or hurt by my sense of humor.\n",
    "    8. I will often get carried away in putting myself down if it makes family or friends laugh.\n",
    "    9. I rarely make other people laugh by telling funny stories about myself.\n",
    "    10. If I am feeling upset or unhappy I usually try to think of something funny about the situation to make myself feel better.\n",
    "    11. When telling jokes or saying funny things, I am usually not concerned about how other people are taking it.\n",
    "    12. I often try to make people like or accept me more by saying something funny about my own weaknesses, blunders, or faults.\n",
    "    13. I laugh and joke a lot with my closest friends.\n",
    "    14. My humorous outlook on life keeps me from getting overly upset or depressed about things.\n",
    "    15. I do not like it when people use humor as a way of criticizing or putting someone down.\n",
    "    16. I don't often say funny things to put myself down.\n",
    "    17. I usually don't like to tell jokes or amuse people.\n",
    "    18. If I'm by myself and I'm feeling unhappy, I make an effort to think of something funny to cheer myself up.\n",
    "    19. Sometimes I think of something that is so funny that I can't stop myself from saying it, even if it is not appropriate for the situation.\n",
    "    20. I often go overboard in putting myself down when I am making jokes or trying to be funny.\n",
    "    21. I enjoy making people laugh.\n",
    "    22. If I am feeling sad or upset, I usually lose my sense of humor.\n",
    "    23. I never participate in laughing at others even if all my friends are doing it.\n",
    "    24. When I am with friends or family, I often seem to be the one that other people make fun of or joke about.\n",
    "    25. I don't often joke around with my friends.\n",
    "    26. It is my experience that thinking about some amusing aspect of a situation is often a very effective way of coping with problems.\n",
    "    27. If I don't like someone, I often use humor or teasing to put them down.\n",
    "    28. If I am having problems or feeling unhappy, I often cover it up by joking around, so that even my closest friends don't know how I really feel.\n",
    "    29. I usually can't think of witty things to say when I'm with other people.\n",
    "    30. I don't need to be with other people to feel amused. I can usually find things to laugh about even when I'm by myself.\n",
    "    31. Even if something is really funny to me, I will not laugh or joke about it if someone will be offended.\n",
    "    32. Letting others laugh at me is my way of keeping my friends and family in good spirits.\n",
    "\n",
    "---\n",
    "\n",
    "### Response scale:\n",
    "\n",
    "For each question, there are 5 possible response codes (\"like scale\") that correspond to different answers. There is also a code that indicates there is no response for that subject.\n",
    "\n",
    "    1 == \"Never or very rarely true\"\n",
    "    2 == \"Rarely true\"\n",
    "    3 == \"Sometimes true\"\n",
    "    4 == \"Often true\"\n",
    "    5 == \"Very often or always true\n",
    "    [-1 == Did not select an answer]\n",
    "    \n",
    "---\n",
    "\n",
    "### Demographics:\n",
    "\n",
    "    age: entered as as text then parsed to an integer.\n",
    "    gender: chosen from drop down list (1=male, 2=female, 3=other, 0=declined)\n",
    "    accuracy: How accurate they thought their answers were on a scale from 0 to 100, answers were entered as text and parsed to an integer. They were instructed to enter a 0 if they did not want to be included in research.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data and perform any EDA and cleaning you think is necessary.\n",
    "\n",
    "It is worth reading over the description of the data columns above for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsq = pd.read_csv('../../../../resource-datasets/humor_styles/hsq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>Q11</th>\n",
       "      <th>Q12</th>\n",
       "      <th>Q13</th>\n",
       "      <th>Q14</th>\n",
       "      <th>Q15</th>\n",
       "      <th>Q16</th>\n",
       "      <th>Q17</th>\n",
       "      <th>Q18</th>\n",
       "      <th>Q19</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q22</th>\n",
       "      <th>Q23</th>\n",
       "      <th>Q24</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q26</th>\n",
       "      <th>Q27</th>\n",
       "      <th>Q28</th>\n",
       "      <th>Q29</th>\n",
       "      <th>Q30</th>\n",
       "      <th>Q31</th>\n",
       "      <th>Q32</th>\n",
       "      <th>affiliative</th>\n",
       "      <th>selfenhancing</th>\n",
       "      <th>agressive</th>\n",
       "      <th>selfdefeating</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  Q11  Q12  Q13  Q14  Q15  Q16  Q17  \\\n",
       "0   2   2   3   1   4   5   4   3   4    3    3    1    5    4    4    4    2   \n",
       "1   2   3   2   2   4   4   4   3   4    3    4    3    3    4    5    4    2   \n",
       "2   3   4   3   3   4   4   3   1   2    4    3    2    4    4    3    3    2   \n",
       "3   3   3   3   4   3   5   4   3  -1    4    2    4    4    5    4    3    3   \n",
       "4   1   4   2   2   3   5   4   1   4    4    2    2    5    4    4    4    2   \n",
       "\n",
       "   Q18  Q19  Q20  Q21  Q22  Q23  Q24  Q25  Q26  Q27  Q28  Q29  Q30  Q31  Q32  \\\n",
       "0    3    3    1    4    4    3    2    1    3    2    4    2    4    2    2   \n",
       "1    2    3    2    3    3    4    2    2    5    1    2    4    4    3    1   \n",
       "2    4    2    1    4    2    4    3    2    4    3    3    2    5    4    2   \n",
       "3    3    3    3    4    3    2    4    2    4    2    2    4    5    3    3   \n",
       "4    3    2    1    5    3    3    1    1    5    2    3    2    5    4    2   \n",
       "\n",
       "   affiliative  selfenhancing  agressive  selfdefeating  age  gender  accuracy  \n",
       "0          4.0            3.5        3.0            2.3   25       2       100  \n",
       "1          3.3            3.5        3.3            2.4   44       2        90  \n",
       "2          3.9            3.9        3.1            2.3   50       1        75  \n",
       "3          3.6            4.0        2.9            3.3   30       2        85  \n",
       "4          4.1            4.1        2.9            2.0   52       1        80  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "hsq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsq.rename(columns={'agressive':'aggressive'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we're only using the Q's then need to filter out the non-answered ones:\n",
    "hsq = hsq.applymap(lambda x: np.nan if x==-1 else x)\n",
    "hsq.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only males and females\n",
    "hsq = hsq[hsq.gender.isin([1,2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up a predictor matrix to predict `gender` (only male vs. female)\n",
    "\n",
    "Choice of predictors is up to you. Justify which variables you include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
       "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
       "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
       "       'Q32', 'affiliative', 'selfenhancing', 'aggressive', 'selfdefeating',\n",
       "       'age', 'gender', 'accuracy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "predictors = [x for x in hsq.columns if 'Q' in x]\n",
    "predictors = predictors + ['accuracy','gender']\n",
    "X = hsq[predictors]\n",
    "y = X.pop('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit a Logistic Regression model and compare your cross-validated accuracy to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.547959\n",
       "2    0.452041\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "baseline = y.value_counts(normalize=True).max()\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58375635, 0.60406091, 0.55102041, 0.60512821, 0.56410256])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's scale predictor:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Xs = pd.DataFrame(scaler.fit_transform(X),columns=X.columns, index=X.index)\n",
    "\n",
    "# then fit the model:\n",
    "logistic = LogisticRegression(C=10**10, solver='lbfgs')\n",
    "logistic.fit(Xs,y)\n",
    "\n",
    "# the cross-validated accuracy:\n",
    "cross_val_score(logistic, Xs, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a 50-50 train-test split. Fit the model on the training data and get the predictions and predicted probabilities on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# make sure to re-scale on the training mean,std\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.6448979591836734\n",
      "Test Score: 0.610204081632653\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(C=10**10,solver='lbfgs')\n",
    "logistic.fit(X_train,y_train)\n",
    "\n",
    "print('Training Score:',logistic.score(X_train,y_train))\n",
    "print('Test Score:',logistic.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
       "       2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "       1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2,\n",
       "       2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n",
       "       1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1,\n",
       "       2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n",
       "       1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2,\n",
       "       1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1,\n",
       "       1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2,\n",
       "       2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now get the predictions for the test data:\n",
    "predictions = logistic.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57424395, 0.42575605],\n",
       "       [0.46616606, 0.53383394],\n",
       "       [0.86903045, 0.13096955],\n",
       "       [0.78161402, 0.21838598],\n",
       "       [0.8524356 , 0.1475644 ],\n",
       "       [0.59958417, 0.40041583],\n",
       "       [0.55269636, 0.44730364],\n",
       "       [0.45085974, 0.54914026],\n",
       "       [0.60838725, 0.39161275],\n",
       "       [0.37424874, 0.62575126],\n",
       "       [0.47300775, 0.52699225],\n",
       "       [0.43433224, 0.56566776],\n",
       "       [0.593925  , 0.406075  ],\n",
       "       [0.34406128, 0.65593872],\n",
       "       [0.50421088, 0.49578912],\n",
       "       [0.62940458, 0.37059542],\n",
       "       [0.73953527, 0.26046473],\n",
       "       [0.74431383, 0.25568617],\n",
       "       [0.81810463, 0.18189537],\n",
       "       [0.67686984, 0.32313016],\n",
       "       [0.40396249, 0.59603751],\n",
       "       [0.74854283, 0.25145717],\n",
       "       [0.46392977, 0.53607023],\n",
       "       [0.51570162, 0.48429838],\n",
       "       [0.33002428, 0.66997572],\n",
       "       [0.4314907 , 0.5685093 ],\n",
       "       [0.39002903, 0.60997097],\n",
       "       [0.75393845, 0.24606155],\n",
       "       [0.31591299, 0.68408701],\n",
       "       [0.51968729, 0.48031271],\n",
       "       [0.85352029, 0.14647971],\n",
       "       [0.78547977, 0.21452023],\n",
       "       [0.50394563, 0.49605437],\n",
       "       [0.76658774, 0.23341226],\n",
       "       [0.67205018, 0.32794982],\n",
       "       [0.42420362, 0.57579638],\n",
       "       [0.35124468, 0.64875532],\n",
       "       [0.59820808, 0.40179192],\n",
       "       [0.4557298 , 0.5442702 ],\n",
       "       [0.53821248, 0.46178752],\n",
       "       [0.67812593, 0.32187407],\n",
       "       [0.59766622, 0.40233378],\n",
       "       [0.59487358, 0.40512642],\n",
       "       [0.44650031, 0.55349969],\n",
       "       [0.44385884, 0.55614116],\n",
       "       [0.47791716, 0.52208284],\n",
       "       [0.55408387, 0.44591613],\n",
       "       [0.75523887, 0.24476113],\n",
       "       [0.47996073, 0.52003927],\n",
       "       [0.35586196, 0.64413804],\n",
       "       [0.40141104, 0.59858896],\n",
       "       [0.43407928, 0.56592072],\n",
       "       [0.29387909, 0.70612091],\n",
       "       [0.83716302, 0.16283698],\n",
       "       [0.71269956, 0.28730044],\n",
       "       [0.34026336, 0.65973664],\n",
       "       [0.61207662, 0.38792338],\n",
       "       [0.81069546, 0.18930454],\n",
       "       [0.37581801, 0.62418199],\n",
       "       [0.36479631, 0.63520369],\n",
       "       [0.61837975, 0.38162025],\n",
       "       [0.37735895, 0.62264105],\n",
       "       [0.25462069, 0.74537931],\n",
       "       [0.39272067, 0.60727933],\n",
       "       [0.43446213, 0.56553787],\n",
       "       [0.60441875, 0.39558125],\n",
       "       [0.54045989, 0.45954011],\n",
       "       [0.76827175, 0.23172825],\n",
       "       [0.34496039, 0.65503961],\n",
       "       [0.6444986 , 0.3555014 ],\n",
       "       [0.67463652, 0.32536348],\n",
       "       [0.56506926, 0.43493074],\n",
       "       [0.32307985, 0.67692015],\n",
       "       [0.24706676, 0.75293324],\n",
       "       [0.57703543, 0.42296457],\n",
       "       [0.87635427, 0.12364573],\n",
       "       [0.48913477, 0.51086523],\n",
       "       [0.4749271 , 0.5250729 ],\n",
       "       [0.52969028, 0.47030972],\n",
       "       [0.56263811, 0.43736189],\n",
       "       [0.65822868, 0.34177132],\n",
       "       [0.67424843, 0.32575157],\n",
       "       [0.36729202, 0.63270798],\n",
       "       [0.57128075, 0.42871925],\n",
       "       [0.47023216, 0.52976784],\n",
       "       [0.64248339, 0.35751661],\n",
       "       [0.50496728, 0.49503272],\n",
       "       [0.4679109 , 0.5320891 ],\n",
       "       [0.47676142, 0.52323858],\n",
       "       [0.48268062, 0.51731938],\n",
       "       [0.26439729, 0.73560271],\n",
       "       [0.69493082, 0.30506918],\n",
       "       [0.58896996, 0.41103004],\n",
       "       [0.70715851, 0.29284149],\n",
       "       [0.3086284 , 0.6913716 ],\n",
       "       [0.63353947, 0.36646053],\n",
       "       [0.46638969, 0.53361031],\n",
       "       [0.59877205, 0.40122795],\n",
       "       [0.44306157, 0.55693843],\n",
       "       [0.66742128, 0.33257872],\n",
       "       [0.67515825, 0.32484175],\n",
       "       [0.3394341 , 0.6605659 ],\n",
       "       [0.46405998, 0.53594002],\n",
       "       [0.36044586, 0.63955414],\n",
       "       [0.47163617, 0.52836383],\n",
       "       [0.64925218, 0.35074782],\n",
       "       [0.15065602, 0.84934398],\n",
       "       [0.33556843, 0.66443157],\n",
       "       [0.77961962, 0.22038038],\n",
       "       [0.4901611 , 0.5098389 ],\n",
       "       [0.57190036, 0.42809964],\n",
       "       [0.67396171, 0.32603829],\n",
       "       [0.64561392, 0.35438608],\n",
       "       [0.59679732, 0.40320268],\n",
       "       [0.75662135, 0.24337865],\n",
       "       [0.40835202, 0.59164798],\n",
       "       [0.47180408, 0.52819592],\n",
       "       [0.65070831, 0.34929169],\n",
       "       [0.73865948, 0.26134052],\n",
       "       [0.26712259, 0.73287741],\n",
       "       [0.7589886 , 0.2410114 ],\n",
       "       [0.60505385, 0.39494615],\n",
       "       [0.46945656, 0.53054344],\n",
       "       [0.66997081, 0.33002919],\n",
       "       [0.49513945, 0.50486055],\n",
       "       [0.5919154 , 0.4080846 ],\n",
       "       [0.7539105 , 0.2460895 ],\n",
       "       [0.7921394 , 0.2078606 ],\n",
       "       [0.69037458, 0.30962542],\n",
       "       [0.71176235, 0.28823765],\n",
       "       [0.51127782, 0.48872218],\n",
       "       [0.31898876, 0.68101124],\n",
       "       [0.26811476, 0.73188524],\n",
       "       [0.54496473, 0.45503527],\n",
       "       [0.49152837, 0.50847163],\n",
       "       [0.31874662, 0.68125338],\n",
       "       [0.5255787 , 0.4744213 ],\n",
       "       [0.63309846, 0.36690154],\n",
       "       [0.48563892, 0.51436108],\n",
       "       [0.34230116, 0.65769884],\n",
       "       [0.22408943, 0.77591057],\n",
       "       [0.37390684, 0.62609316],\n",
       "       [0.69172686, 0.30827314],\n",
       "       [0.73165801, 0.26834199],\n",
       "       [0.43778475, 0.56221525],\n",
       "       [0.72637731, 0.27362269],\n",
       "       [0.75437783, 0.24562217],\n",
       "       [0.63388122, 0.36611878],\n",
       "       [0.51540857, 0.48459143],\n",
       "       [0.36137403, 0.63862597],\n",
       "       [0.83360537, 0.16639463],\n",
       "       [0.6264335 , 0.3735665 ],\n",
       "       [0.74746308, 0.25253692],\n",
       "       [0.52374543, 0.47625457],\n",
       "       [0.66333441, 0.33666559],\n",
       "       [0.33569957, 0.66430043],\n",
       "       [0.35197647, 0.64802353],\n",
       "       [0.3904924 , 0.6095076 ],\n",
       "       [0.62695045, 0.37304955],\n",
       "       [0.33051049, 0.66948951],\n",
       "       [0.78436961, 0.21563039],\n",
       "       [0.66149637, 0.33850363],\n",
       "       [0.17950415, 0.82049585],\n",
       "       [0.58768701, 0.41231299],\n",
       "       [0.64887218, 0.35112782],\n",
       "       [0.28509792, 0.71490208],\n",
       "       [0.35882279, 0.64117721],\n",
       "       [0.727125  , 0.272875  ],\n",
       "       [0.74063908, 0.25936092],\n",
       "       [0.86124134, 0.13875866],\n",
       "       [0.23557613, 0.76442387],\n",
       "       [0.79261924, 0.20738076],\n",
       "       [0.55224918, 0.44775082],\n",
       "       [0.5514143 , 0.4485857 ],\n",
       "       [0.51284753, 0.48715247],\n",
       "       [0.38441859, 0.61558141],\n",
       "       [0.68353605, 0.31646395],\n",
       "       [0.81254959, 0.18745041],\n",
       "       [0.55746364, 0.44253636],\n",
       "       [0.70953871, 0.29046129],\n",
       "       [0.45929062, 0.54070938],\n",
       "       [0.51719942, 0.48280058],\n",
       "       [0.74249623, 0.25750377],\n",
       "       [0.77683459, 0.22316541],\n",
       "       [0.71258188, 0.28741812],\n",
       "       [0.44860562, 0.55139438],\n",
       "       [0.69784173, 0.30215827],\n",
       "       [0.61684803, 0.38315197],\n",
       "       [0.64151053, 0.35848947],\n",
       "       [0.46263217, 0.53736783],\n",
       "       [0.62252149, 0.37747851],\n",
       "       [0.45842608, 0.54157392],\n",
       "       [0.54211993, 0.45788007],\n",
       "       [0.78532217, 0.21467783],\n",
       "       [0.46115933, 0.53884067],\n",
       "       [0.66811957, 0.33188043],\n",
       "       [0.2530636 , 0.7469364 ],\n",
       "       [0.90244182, 0.09755818],\n",
       "       [0.48061702, 0.51938298],\n",
       "       [0.51712403, 0.48287597],\n",
       "       [0.38255446, 0.61744554],\n",
       "       [0.6721193 , 0.3278807 ],\n",
       "       [0.64106179, 0.35893821],\n",
       "       [0.20473923, 0.79526077],\n",
       "       [0.81931215, 0.18068785],\n",
       "       [0.26819588, 0.73180412],\n",
       "       [0.71635231, 0.28364769],\n",
       "       [0.58441641, 0.41558359],\n",
       "       [0.77267522, 0.22732478],\n",
       "       [0.51778132, 0.48221868],\n",
       "       [0.87721651, 0.12278349],\n",
       "       [0.45247841, 0.54752159],\n",
       "       [0.74457181, 0.25542819],\n",
       "       [0.56280261, 0.43719739],\n",
       "       [0.54676475, 0.45323525],\n",
       "       [0.48833806, 0.51166194],\n",
       "       [0.65004696, 0.34995304],\n",
       "       [0.64353884, 0.35646116],\n",
       "       [0.82212531, 0.17787469],\n",
       "       [0.55907846, 0.44092154],\n",
       "       [0.67074575, 0.32925425],\n",
       "       [0.75624656, 0.24375344],\n",
       "       [0.63834025, 0.36165975],\n",
       "       [0.48279504, 0.51720496],\n",
       "       [0.45971215, 0.54028785],\n",
       "       [0.53586585, 0.46413415],\n",
       "       [0.58987328, 0.41012672],\n",
       "       [0.60759392, 0.39240608],\n",
       "       [0.71275652, 0.28724348],\n",
       "       [0.34541093, 0.65458907],\n",
       "       [0.45432756, 0.54567244],\n",
       "       [0.68778623, 0.31221377],\n",
       "       [0.67131171, 0.32868829],\n",
       "       [0.57441468, 0.42558532],\n",
       "       [0.44304373, 0.55695627],\n",
       "       [0.69903774, 0.30096226],\n",
       "       [0.49109234, 0.50890766],\n",
       "       [0.53491625, 0.46508375],\n",
       "       [0.20709419, 0.79290581],\n",
       "       [0.71164259, 0.28835741],\n",
       "       [0.40274278, 0.59725722],\n",
       "       [0.77789551, 0.22210449],\n",
       "       [0.53014474, 0.46985526],\n",
       "       [0.526306  , 0.473694  ],\n",
       "       [0.26768786, 0.73231214],\n",
       "       [0.6303104 , 0.3696896 ],\n",
       "       [0.33544449, 0.66455551],\n",
       "       [0.25387722, 0.74612278],\n",
       "       [0.32211413, 0.67788587],\n",
       "       [0.52752638, 0.47247362],\n",
       "       [0.36006644, 0.63993356],\n",
       "       [0.61113513, 0.38886487],\n",
       "       [0.4544559 , 0.5455441 ],\n",
       "       [0.59981157, 0.40018843],\n",
       "       [0.78044643, 0.21955357],\n",
       "       [0.63135358, 0.36864642],\n",
       "       [0.70263761, 0.29736239],\n",
       "       [0.66680115, 0.33319885],\n",
       "       [0.46711778, 0.53288222],\n",
       "       [0.7913902 , 0.2086098 ],\n",
       "       [0.32542657, 0.67457343],\n",
       "       [0.35166978, 0.64833022],\n",
       "       [0.17738917, 0.82261083],\n",
       "       [0.31434804, 0.68565196],\n",
       "       [0.61533445, 0.38466555],\n",
       "       [0.73650596, 0.26349404],\n",
       "       [0.35371581, 0.64628419],\n",
       "       [0.6478325 , 0.3521675 ],\n",
       "       [0.46353593, 0.53646407],\n",
       "       [0.75519847, 0.24480153],\n",
       "       [0.16434437, 0.83565563],\n",
       "       [0.1739547 , 0.8260453 ],\n",
       "       [0.60068576, 0.39931424],\n",
       "       [0.78577957, 0.21422043],\n",
       "       [0.637038  , 0.362962  ],\n",
       "       [0.56873563, 0.43126437],\n",
       "       [0.29298809, 0.70701191],\n",
       "       [0.6447419 , 0.3552581 ],\n",
       "       [0.43891509, 0.56108491],\n",
       "       [0.70364661, 0.29635339],\n",
       "       [0.35773134, 0.64226866],\n",
       "       [0.59025549, 0.40974451],\n",
       "       [0.29526077, 0.70473923],\n",
       "       [0.42289973, 0.57710027],\n",
       "       [0.83213101, 0.16786899],\n",
       "       [0.18465774, 0.81534226],\n",
       "       [0.54428306, 0.45571694],\n",
       "       [0.62780933, 0.37219067],\n",
       "       [0.61868062, 0.38131938],\n",
       "       [0.61900502, 0.38099498],\n",
       "       [0.336188  , 0.663812  ],\n",
       "       [0.72520457, 0.27479543],\n",
       "       [0.44054253, 0.55945747],\n",
       "       [0.44140455, 0.55859545],\n",
       "       [0.63642911, 0.36357089],\n",
       "       [0.53495971, 0.46504029],\n",
       "       [0.50498097, 0.49501903],\n",
       "       [0.37911495, 0.62088505],\n",
       "       [0.66192792, 0.33807208],\n",
       "       [0.77592809, 0.22407191],\n",
       "       [0.32322024, 0.67677976],\n",
       "       [0.20646419, 0.79353581],\n",
       "       [0.74608437, 0.25391563],\n",
       "       [0.78206339, 0.21793661],\n",
       "       [0.33175846, 0.66824154],\n",
       "       [0.39328056, 0.60671944],\n",
       "       [0.70054017, 0.29945983],\n",
       "       [0.8509884 , 0.1490116 ],\n",
       "       [0.87148748, 0.12851252],\n",
       "       [0.70319295, 0.29680705],\n",
       "       [0.53294279, 0.46705721],\n",
       "       [0.78028625, 0.21971375],\n",
       "       [0.8095881 , 0.1904119 ],\n",
       "       [0.74075065, 0.25924935],\n",
       "       [0.83088039, 0.16911961],\n",
       "       [0.5049237 , 0.4950763 ],\n",
       "       [0.50107008, 0.49892992],\n",
       "       [0.77369068, 0.22630932],\n",
       "       [0.70558858, 0.29441142],\n",
       "       [0.2212397 , 0.7787603 ],\n",
       "       [0.4988043 , 0.5011957 ],\n",
       "       [0.73884269, 0.26115731],\n",
       "       [0.42885394, 0.57114606],\n",
       "       [0.50344378, 0.49655622],\n",
       "       [0.29129973, 0.70870027],\n",
       "       [0.63605296, 0.36394704],\n",
       "       [0.79320076, 0.20679924],\n",
       "       [0.86133497, 0.13866503],\n",
       "       [0.47647232, 0.52352768],\n",
       "       [0.71897873, 0.28102127],\n",
       "       [0.66303552, 0.33696448],\n",
       "       [0.2331036 , 0.7668964 ],\n",
       "       [0.36957225, 0.63042775],\n",
       "       [0.38263729, 0.61736271],\n",
       "       [0.78553766, 0.21446234],\n",
       "       [0.7023589 , 0.2976411 ],\n",
       "       [0.47050507, 0.52949493],\n",
       "       [0.46316152, 0.53683848],\n",
       "       [0.59995657, 0.40004343],\n",
       "       [0.3858404 , 0.6141596 ],\n",
       "       [0.23876997, 0.76123003],\n",
       "       [0.65929317, 0.34070683],\n",
       "       [0.15058089, 0.84941911],\n",
       "       [0.86835293, 0.13164707],\n",
       "       [0.48249913, 0.51750087],\n",
       "       [0.73867051, 0.26132949],\n",
       "       [0.63964783, 0.36035217],\n",
       "       [0.68234906, 0.31765094],\n",
       "       [0.42148681, 0.57851319],\n",
       "       [0.33982716, 0.66017284],\n",
       "       [0.82557514, 0.17442486],\n",
       "       [0.4557177 , 0.5442823 ],\n",
       "       [0.29712886, 0.70287114],\n",
       "       [0.54720539, 0.45279461],\n",
       "       [0.33098783, 0.66901217],\n",
       "       [0.37579052, 0.62420948],\n",
       "       [0.54346323, 0.45653677],\n",
       "       [0.75316772, 0.24683228],\n",
       "       [0.68295601, 0.31704399],\n",
       "       [0.7075524 , 0.2924476 ],\n",
       "       [0.80640474, 0.19359526],\n",
       "       [0.69708999, 0.30291001],\n",
       "       [0.71974715, 0.28025285],\n",
       "       [0.49936248, 0.50063752],\n",
       "       [0.76246201, 0.23753799],\n",
       "       [0.61115936, 0.38884064],\n",
       "       [0.424897  , 0.575103  ],\n",
       "       [0.37360597, 0.62639403],\n",
       "       [0.67229177, 0.32770823],\n",
       "       [0.38217395, 0.61782605],\n",
       "       [0.66271077, 0.33728923],\n",
       "       [0.50317389, 0.49682611],\n",
       "       [0.75225996, 0.24774004],\n",
       "       [0.66989885, 0.33010115],\n",
       "       [0.73159072, 0.26840928],\n",
       "       [0.57307443, 0.42692557],\n",
       "       [0.5084303 , 0.4915697 ],\n",
       "       [0.34909026, 0.65090974],\n",
       "       [0.3107909 , 0.6892091 ],\n",
       "       [0.37822135, 0.62177865],\n",
       "       [0.30988778, 0.69011222],\n",
       "       [0.77941007, 0.22058993],\n",
       "       [0.80409611, 0.19590389],\n",
       "       [0.75191017, 0.24808983],\n",
       "       [0.58041697, 0.41958303],\n",
       "       [0.54912985, 0.45087015],\n",
       "       [0.36539846, 0.63460154],\n",
       "       [0.8302574 , 0.1697426 ],\n",
       "       [0.64510642, 0.35489358],\n",
       "       [0.68083461, 0.31916539],\n",
       "       [0.49482165, 0.50517835],\n",
       "       [0.79409078, 0.20590922],\n",
       "       [0.53285289, 0.46714711],\n",
       "       [0.52530028, 0.47469972],\n",
       "       [0.53490707, 0.46509293],\n",
       "       [0.7459199 , 0.2540801 ],\n",
       "       [0.70244987, 0.29755013],\n",
       "       [0.77203308, 0.22796692],\n",
       "       [0.57467845, 0.42532155],\n",
       "       [0.35457425, 0.64542575],\n",
       "       [0.49727747, 0.50272253],\n",
       "       [0.53158022, 0.46841978],\n",
       "       [0.35268679, 0.64731321],\n",
       "       [0.66630161, 0.33369839],\n",
       "       [0.3359419 , 0.6640581 ],\n",
       "       [0.55331395, 0.44668605],\n",
       "       [0.54475681, 0.45524319],\n",
       "       [0.41224492, 0.58775508],\n",
       "       [0.27641787, 0.72358213],\n",
       "       [0.48976505, 0.51023495],\n",
       "       [0.68816057, 0.31183943],\n",
       "       [0.27594976, 0.72405024],\n",
       "       [0.53556553, 0.46443447],\n",
       "       [0.3596276 , 0.6403724 ],\n",
       "       [0.81684924, 0.18315076],\n",
       "       [0.656693  , 0.343307  ],\n",
       "       [0.38794457, 0.61205543],\n",
       "       [0.87169501, 0.12830499],\n",
       "       [0.17113305, 0.82886695],\n",
       "       [0.37192867, 0.62807133],\n",
       "       [0.33213363, 0.66786637],\n",
       "       [0.36513301, 0.63486699],\n",
       "       [0.33336896, 0.66663104],\n",
       "       [0.66132224, 0.33867776],\n",
       "       [0.66113199, 0.33886801],\n",
       "       [0.57384493, 0.42615507],\n",
       "       [0.68477564, 0.31522436],\n",
       "       [0.78236911, 0.21763089],\n",
       "       [0.25770795, 0.74229205],\n",
       "       [0.48421026, 0.51578974],\n",
       "       [0.38202433, 0.61797567],\n",
       "       [0.52665153, 0.47334847],\n",
       "       [0.6605343 , 0.3394657 ],\n",
       "       [0.59163131, 0.40836869],\n",
       "       [0.37206048, 0.62793952],\n",
       "       [0.56182961, 0.43817039],\n",
       "       [0.72014376, 0.27985624],\n",
       "       [0.16051377, 0.83948623],\n",
       "       [0.5917506 , 0.4082494 ],\n",
       "       [0.72160923, 0.27839077],\n",
       "       [0.24592837, 0.75407163],\n",
       "       [0.42895074, 0.57104926],\n",
       "       [0.42709527, 0.57290473],\n",
       "       [0.36826185, 0.63173815],\n",
       "       [0.72073089, 0.27926911],\n",
       "       [0.64761609, 0.35238391],\n",
       "       [0.2513505 , 0.7486495 ],\n",
       "       [0.40258307, 0.59741693],\n",
       "       [0.67742149, 0.32257851],\n",
       "       [0.65467718, 0.34532282],\n",
       "       [0.44221072, 0.55778928],\n",
       "       [0.43541629, 0.56458371],\n",
       "       [0.52706675, 0.47293325],\n",
       "       [0.64896336, 0.35103664],\n",
       "       [0.36793458, 0.63206542],\n",
       "       [0.6272343 , 0.3727657 ],\n",
       "       [0.46804488, 0.53195512],\n",
       "       [0.25299103, 0.74700897],\n",
       "       [0.44246789, 0.55753211],\n",
       "       [0.34152684, 0.65847316],\n",
       "       [0.41567262, 0.58432738],\n",
       "       [0.4533236 , 0.5466764 ],\n",
       "       [0.53625828, 0.46374172],\n",
       "       [0.38010194, 0.61989806],\n",
       "       [0.63590538, 0.36409462],\n",
       "       [0.49926855, 0.50073145],\n",
       "       [0.59994799, 0.40005201],\n",
       "       [0.56455705, 0.43544295],\n",
       "       [0.52490876, 0.47509124],\n",
       "       [0.29224674, 0.70775326],\n",
       "       [0.46740629, 0.53259371],\n",
       "       [0.52946665, 0.47053335],\n",
       "       [0.37198543, 0.62801457],\n",
       "       [0.35661948, 0.64338052],\n",
       "       [0.700279  , 0.299721  ],\n",
       "       [0.47305268, 0.52694732],\n",
       "       [0.61487271, 0.38512729],\n",
       "       [0.2382736 , 0.7617264 ],\n",
       "       [0.66330902, 0.33669098],\n",
       "       [0.69511683, 0.30488317],\n",
       "       [0.84176384, 0.15823616],\n",
       "       [0.82920856, 0.17079144],\n",
       "       [0.33959267, 0.66040733],\n",
       "       [0.67230841, 0.32769159],\n",
       "       [0.85448722, 0.14551278],\n",
       "       [0.67354466, 0.32645534],\n",
       "       [0.54513236, 0.45486764],\n",
       "       [0.42695198, 0.57304802],\n",
       "       [0.5138203 , 0.4861797 ],\n",
       "       [0.71460004, 0.28539996]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now get the predictions for the test data:\n",
    "logistic.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Manually calculate the true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.536735\n",
       "2    0.463265\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "# true positives will always be the most numerous class in the modelled training set,\n",
    "# so in this case there are more of class 1 so that's +ve.\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 187\n",
      "fp: 104\n",
      "tn: 112\n",
      "fn: 87\n"
     ]
    }
   ],
   "source": [
    "tp = np.sum((y_test == 1) & (predictions == 1))\n",
    "fp = np.sum((y_test == 2) & (predictions == 1))\n",
    "tn = np.sum((y_test == 2) & (predictions == 2))\n",
    "fn = np.sum((y_test == 1) & (predictions == 2))\n",
    "print(\"tp:\", tp)\n",
    "print(\"fp:\", fp)\n",
    "print(\"tn:\", tn)\n",
    "print(\"fn:\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Construct the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[187  87]\n",
      " [104 112]]\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Print out the false positive count as you change your threshold for predicting label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-9e87b4a6d2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# first create a df from predict_proba... MAKE SURE TO ADD ORIGINAL INDEX WHEN CREATING NEW DATAFRAMES,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# AS THESE NEW DF'S WILL HAVE THEIR OWN NEW INDEX!!! MAKING COMPARISON LATER IMPOSSIBLE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mY_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_1_pp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'class_2_pp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mY_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "# first create a df from predict_proba... MAKE SURE TO ADD ORIGINAL INDEX WHEN CREATING NEW DATAFRAMES,\n",
    "# AS THESE NEW DF'S WILL HAVE THEIR OWN NEW INDEX!!! MAKING COMPARISON LATER IMPOSSIBLE.\n",
    "Y_pp = pd.DataFrame(logistic.predict_proba(X_test),columns=['class_1_pp','class_2_pp'], index=X_test.index)\n",
    "Y_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(x, threshold):\n",
    "    if x >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pp['predict_at_threshold'] = Y_pp.class_1_pp.apply(predict_at_threshold, threshold=0.5)\n",
    "np.sum((y_test == 2) & (Y_pp['predict_at_threshold'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 215, 215, 214, 214, 213, 213, 213, 213, 213, 213, 211, 206, 205, 204, 203, 197, 197, 195, 191, 186, 179, 176, 173, 169, 165, 157, 150, 146, 143, 137, 133, 126, 121, 118, 113, 106, 103, 100, 97, 92, 87, 82, 79, 74, 66, 58, 56, 51, 47, 46, 41, 35, 31, 29, 28, 21, 19, 14, 11, 9, 6, 6, 6, 4, 3, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "fp_list = []\n",
    "for i in range(0, 100):\n",
    "    Y_pp['predict_at_threshold'] = Y_pp.class_1_pp.apply(predict_at_threshold, threshold=i/100)\n",
    "    fp_list.append(np.sum((y_test == 2) & (Y_pp['predict_at_threshold'] == 1)))\n",
    "print(\"fp:\", fp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot an ROC curve using your predicted probabilities on the test data.\n",
    "\n",
    "Calculate the area under the curve.\n",
    "\n",
    "> *Hint: go back to the lesson to find code for plotting the ROC curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is not binary and pos_label is not specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-fc70a060be17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# A:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# For class 1, find the area under the curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_1_pp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \"\"\"\n\u001b[1;32m    621\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 622\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    413\u001b[0m              \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m              np.array_equal(classes, [1]))):\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data is not binary and pos_label is not specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is not binary and pos_label is not specified"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "# For class 1, find the area under the curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, Y_pp.class_1_pp)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot of a ROC curve for class 1\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('ROC curve', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Cross-validate a logistic regression with a Ridge penalty.\n",
    "\n",
    "Logistic regression can also use the Ridge penalty. Sklearn's [`LogisticRegressionCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) class will help you cross-validate an appropriate regularization strength.\n",
    "\n",
    "**Important `LogisticRegressionCV` arguments:**\n",
    "- `penalty`: this can be one of `'l1'` or `'l2'`. L1 is the Lasso, and L2 is the Ridge.\n",
    "- `Cs`: How many different (automatically-selected) regularization strengths should be tested.\n",
    "- `cv`: How many cross-validation folds should be used to test regularization strength.\n",
    "- `solver`: When using the lasso penalty, this should be set to `'liblinear'`\n",
    "\n",
    "> **Note:** The `C` regularization strength is the *inverse* of alpha. That is to say, `C = 1./alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.A Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.B Construct the confusion matrix for the Ridge LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Plot the ROC curve for the original and Ridge logistic regressions on the same plot.\n",
    "\n",
    "Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Cross-validate a Lasso logistic regression.\n",
    "\n",
    "**Hint:**\n",
    "- `penalty` must be set to `'l1'`\n",
    "- `solver` must be set to `'liblinear'`\n",
    "\n",
    "> **Note:** The lasso penalty can be considerably slower. You may want to try fewer Cs or use fewer cv folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Make the confusion matrix for the Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Plot all three logistic regression models on the same ROC plot.\n",
    "\n",
    "Which is the best (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Look at the coefficients for the Lasso logistic regression model. Which variables are the most important ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
